
==> Audit <==
|------------|---------------------------|----------|-----------------------|---------|----------------------|----------------------|
|  Command   |           Args            | Profile  |         User          | Version |      Start Time      |       End Time       |
|------------|---------------------------|----------|-----------------------|---------|----------------------|----------------------|
| start      |                           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 10:16 CEST | 02 May 24 10:18 CEST |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 10:38 CEST | 02 May 24 10:38 CEST |
| service    | php-service               | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 10:42 CEST |                      |
| delete     | service php-service       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 10:54 CEST |                      |
| delete     | deployment php-deployment | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 10:55 CEST |                      |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 10:55 CEST | 02 May 24 10:56 CEST |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 11:00 CEST | 02 May 24 11:00 CEST |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 11:29 CEST | 02 May 24 11:29 CEST |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 11:32 CEST | 02 May 24 11:32 CEST |
| service    | symfony-service           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 11:40 CEST | 02 May 24 11:44 CEST |
| service    | symfony-service           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 11:44 CEST | 02 May 24 11:44 CEST |
| service    | symfony-service           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 12:07 CEST | 02 May 24 12:08 CEST |
| service    | symfony-service           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 12:10 CEST | 02 May 24 12:11 CEST |
| service    | symfony-service           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 12:16 CEST | 02 May 24 12:16 CEST |
| service    | symfony-service           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 12:21 CEST |                      |
| service    | symfony-service           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 13:10 CEST | 02 May 24 13:38 CEST |
| service    | symfony-service           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 13:38 CEST | 02 May 24 13:39 CEST |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 13:44 CEST | 02 May 24 13:44 CEST |
| service    | symfony-service           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 13:52 CEST |                      |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 13:54 CEST | 02 May 24 13:54 CEST |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 14:30 CEST | 02 May 24 14:30 CEST |
| service    | symfony-service           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 14:30 CEST | 02 May 24 14:43 CEST |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 14:45 CEST | 02 May 24 14:45 CEST |
| addons     | enable nginx-ingress      | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 14:53 CEST |                      |
| addons     | enable ingress            | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 14:53 CEST |                      |
| addons     | enable ingress            | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 14:54 CEST | 02 May 24 14:54 CEST |
| ip         |                           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 14:55 CEST | 02 May 24 14:55 CEST |
| service    | app-service               | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 15:24 CEST | 02 May 24 15:29 CEST |
| service    | app2-service              | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 15:24 CEST | 02 May 24 15:29 CEST |
| service    | app2-service              | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 15:30 CEST | 02 May 24 16:27 CEST |
| service    | app-service               | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 15:31 CEST | 02 May 24 16:26 CEST |
| tunnel     |                           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 15:31 CEST | 02 May 24 16:26 CEST |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 19:34 CEST | 02 May 24 19:34 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 19:46 CEST |                      |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 19:49 CEST |                      |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 19:51 CEST |                      |
| service    | wordpress-mysql           | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 02 May 24 19:51 CEST | 02 May 24 19:52 CEST |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 03 May 24 11:57 CEST | 03 May 24 11:57 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 03 May 24 15:02 CEST | 03 May 24 15:03 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 03 May 24 15:04 CEST | 03 May 24 15:05 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 03 May 24 15:14 CEST | 03 May 24 15:22 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 03 May 24 15:26 CEST | 03 May 24 15:27 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 03 May 24 15:33 CEST | 03 May 24 15:33 CEST |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 03 May 24 15:56 CEST | 03 May 24 15:56 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 03 May 24 16:11 CEST | 03 May 24 16:13 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 03 May 24 16:16 CEST | 05 May 24 01:43 CEST |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 00:03 CEST | 05 May 24 00:03 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 01:31 CEST | 05 May 24 01:32 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 01:32 CEST | 05 May 24 01:32 CEST |
| docker-env | minikube docker-env       | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 01:42 CEST | 05 May 24 01:42 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 01:42 CEST | 05 May 24 01:43 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 01:43 CEST | 05 May 24 01:45 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 01:47 CEST | 05 May 24 01:47 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 01:49 CEST | 05 May 24 02:13 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 02:08 CEST | 05 May 24 02:09 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 02:10 CEST | 05 May 24 02:10 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 02:18 CEST | 05 May 24 02:31 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 02:31 CEST | 05 May 24 02:33 CEST |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 02:45 CEST |                      |
| service    | wordpress                 | minikube | DESKTOP-MFUERJT\admin | v1.33.0 | 05 May 24 02:48 CEST |                      |
|------------|---------------------------|----------|-----------------------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2024/05/02 10:16:45
Running on machine: DESKTOP-MFUERJT
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0502 10:16:45.117227   39300 out.go:291] Setting OutFile to fd 464 ...
I0502 10:16:45.117760   39300 out.go:343] isatty.IsTerminal(464) = true
I0502 10:16:45.117760   39300 out.go:304] Setting ErrFile to fd 464...
I0502 10:16:45.117832   39300 out.go:343] isatty.IsTerminal(464) = true
I0502 10:16:45.159382   39300 out.go:298] Setting JSON to false
I0502 10:16:45.166084   39300 start.go:129] hostinfo: {"hostname":"DESKTOP-MFUERJT","uptime":248991,"bootTime":1714388813,"procs":360,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.3447 Build 22631.3447","kernelVersion":"10.0.22631.3447 Build 22631.3447","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"c2f70919-a955-4699-8f56-bf68998e77d3"}
W0502 10:16:45.166084   39300 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0502 10:16:45.168794   39300 out.go:177] 😄  minikube v1.33.0 on Microsoft Windows 11 Pro 10.0.22631.3447 Build 22631.3447
I0502 10:16:45.169920   39300 notify.go:220] Checking for updates...
I0502 10:16:45.170989   39300 driver.go:392] Setting default libvirt URI to qemu:///system
I0502 10:16:45.171064   39300 global.go:112] Querying for installed drivers using PATH=C:\Users\admin\bin;C:\Program Files\Git\mingw64\bin;C:\Program Files\Git\usr\local\bin;C:\Program Files\Git\usr\bin;C:\Program Files\Git\usr\bin;C:\Program Files\Git\mingw64\bin;C:\Program Files\Git\usr\bin;C:\Users\admin\bin;C:\Program Files (x86)\VMware\VMware Workstation\bin;C:\Program Files (x86)\Eclipse Adoptium\jdk-17.0.9.9-hotspot\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0;C:\WINDOWS\System32\OpenSSH;C:\Program Files\PuTTY;C:\Program Files\Git\cmd;C:\Program Files\Docker\Docker\resources\bin;C:\minikube;C:\Users\admin\AppData\Local\Microsoft\WindowsApps;C:\Users\admin\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\admin\AppData\Local\GitHubDesktop\bin;C:\Program Files\Git\usr\bin\vendor_perl;C:\Program Files\Git\usr\bin\core_perl
I0502 10:16:45.393622   39300 docker.go:122] docker version: linux-26.0.0:Docker Desktop 4.29.0 (145265)
I0502 10:16:45.399284   39300 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0502 10:16:49.037194   39300 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (3.6378949s)
I0502 10:16:49.037736   39300 info.go:266] docker info: {ID:a414024f-4433-4525-8cf3-fb6791f42195 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:54 OomKillDisable:true NGoroutines:76 SystemTime:2024-05-02 08:16:49.002339859 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:16 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:16694267904 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.27] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.6.3]] Warnings:<nil>}}
I0502 10:16:49.037736   39300 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0502 10:16:49.049647   39300 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0502 10:16:49.049647   39300 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0502 10:16:50.709169   39300 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:Hyper-V requires Administrator privileges Reason: Fix:Right-click the PowerShell icon and select Run as Administrator to open PowerShell in elevated mode. Doc: Version:}
I0502 10:16:50.720609   39300 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0502 10:16:50.739895   39300 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0502 10:16:50.750753   39300 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0502 10:16:50.750753   39300 driver.go:314] not recommending "ssh" due to default: false
I0502 10:16:50.750753   39300 driver.go:309] not recommending "hyperv" due to health: Hyper-V requires Administrator privileges
I0502 10:16:50.750753   39300 driver.go:349] Picked: docker
I0502 10:16:50.750753   39300 driver.go:350] Alternatives: [ssh]
I0502 10:16:50.750753   39300 driver.go:351] Rejects: [podman hyperv qemu2 virtualbox vmware]
I0502 10:16:50.751810   39300 out.go:177] ✨  Automatically selected the docker driver
I0502 10:16:50.753405   39300 start.go:297] selected driver: docker
I0502 10:16:50.753405   39300 start.go:901] validating driver "docker" against <nil>
I0502 10:16:50.753405   39300 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0502 10:16:50.776733   39300 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0502 10:16:51.418394   39300 info.go:266] docker info: {ID:a414024f-4433-4525-8cf3-fb6791f42195 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:54 OomKillDisable:true NGoroutines:76 SystemTime:2024-05-02 08:16:51.387241285 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:16 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:16694267904 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.27] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.6.3]] Warnings:<nil>}}
I0502 10:16:51.418943   39300 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0502 10:16:51.462188   39300 start_flags.go:393] Using suggested 8100MB memory alloc based on sys=32615MB, container=15920MB
I0502 10:16:51.463031   39300 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0502 10:16:51.466232   39300 out.go:177] 📌  Using Docker Desktop driver with root privileges
I0502 10:16:51.467295   39300 cni.go:84] Creating CNI manager for ""
I0502 10:16:51.467295   39300 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0502 10:16:51.467295   39300 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0502 10:16:51.467915   39300 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0502 10:16:51.469997   39300 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0502 10:16:51.471058   39300 cache.go:121] Beginning downloading kic base image for docker with docker
I0502 10:16:51.472118   39300 out.go:177] 🚜  Pulling base image v0.0.43 ...
I0502 10:16:51.476016   39300 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0502 10:16:51.476016   39300 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0502 10:16:51.476545   39300 preload.go:147] Found local preload: C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0502 10:16:51.476545   39300 cache.go:56] Caching tarball of preloaded images
I0502 10:16:51.476996   39300 preload.go:173] Found C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0502 10:16:51.476996   39300 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0502 10:16:51.477503   39300 profile.go:143] Saving config to C:\Users\admin\.minikube\profiles\minikube\config.json ...
I0502 10:16:51.477503   39300 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\config.json: {Name:mk0438f7842862f34f2971e6d7deebd1156bea97 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0502 10:16:51.656002   39300 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon, skipping pull
I0502 10:16:51.656002   39300 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 exists in daemon, skipping load
I0502 10:16:51.656002   39300 cache.go:194] Successfully downloaded all kic artifacts
I0502 10:16:51.656507   39300 start.go:360] acquireMachinesLock for minikube: {Name:mk643f7e9da83601d587a510a4be0c611c47f2bc Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0502 10:16:51.656507   39300 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0502 10:16:51.656507   39300 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0502 10:16:51.657035   39300 start.go:125] createHost starting for "" (driver="docker")
I0502 10:16:51.658633   39300 out.go:204] 🔥  Creating docker container (CPUs=2, Memory=8100MB) ...
I0502 10:16:51.659164   39300 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0502 10:16:51.659164   39300 client.go:168] LocalClient.Create starting
I0502 10:16:51.659693   39300 main.go:141] libmachine: Reading certificate data from C:\Users\admin\.minikube\certs\ca.pem
I0502 10:16:51.666698   39300 main.go:141] libmachine: Decoding PEM data...
I0502 10:16:51.666698   39300 main.go:141] libmachine: Parsing certificate...
I0502 10:16:51.666816   39300 main.go:141] libmachine: Reading certificate data from C:\Users\admin\.minikube\certs\cert.pem
I0502 10:16:51.674460   39300 main.go:141] libmachine: Decoding PEM data...
I0502 10:16:51.674460   39300 main.go:141] libmachine: Parsing certificate...
I0502 10:16:51.689184   39300 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0502 10:16:51.841856   39300 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0502 10:16:51.848992   39300 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0502 10:16:51.848992   39300 cli_runner.go:164] Run: docker network inspect minikube
W0502 10:16:52.016096   39300 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0502 10:16:52.016096   39300 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0502 10:16:52.016096   39300 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0502 10:16:52.024223   39300 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0502 10:16:52.332569   39300 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc002444600}
I0502 10:16:52.332569   39300 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0502 10:16:52.339836   39300 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0502 10:16:52.570293   39300 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0502 10:16:52.570293   39300 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0502 10:16:52.582755   39300 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0502 10:16:52.743675   39300 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0502 10:16:52.898388   39300 oci.go:103] Successfully created a docker volume minikube
I0502 10:16:52.910485   39300 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib
I0502 10:16:56.320900   39300 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib: (3.4104007s)
I0502 10:16:56.320900   39300 oci.go:107] Successfully prepared a docker volume minikube
I0502 10:16:56.320900   39300 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0502 10:16:56.320900   39300 kic.go:194] Starting extracting preloaded images to volume ...
I0502 10:16:56.339970   39300 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir
I0502 10:17:34.501463   39300 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir: (38.161332s)
I0502 10:17:34.501463   39300 kic.go:203] duration metric: took 38.1804026s to extract preloaded images to volume ...
I0502 10:17:34.510503   39300 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0502 10:17:35.930753   39300 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.4202441s)
I0502 10:17:35.931755   39300 info.go:266] docker info: {ID:a414024f-4433-4525-8cf3-fb6791f42195 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:55 OomKillDisable:true NGoroutines:76 SystemTime:2024-05-02 08:17:35.842553735 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:16 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:16694267904 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.27] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.6.3]] Warnings:<nil>}}
I0502 10:17:35.940552   39300 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0502 10:17:36.778449   39300 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=8100mb --memory-swap=8100mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737
I0502 10:17:38.346916   39300 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=8100mb --memory-swap=8100mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737: (1.5684604s)
I0502 10:17:38.360214   39300 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0502 10:17:38.727014   39300 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0502 10:17:38.965513   39300 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0502 10:17:39.678158   39300 oci.go:144] the created container "minikube" has a running status.
I0502 10:17:39.678158   39300 kic.go:225] Creating ssh key for kic: C:\Users\admin\.minikube\machines\minikube\id_rsa...
I0502 10:17:39.850816   39300 kic_runner.go:191] docker (temp): C:\Users\admin\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0502 10:17:40.462402   39300 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0502 10:17:40.876935   39300 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0502 10:17:40.876935   39300 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0502 10:17:41.143915   39300 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\admin\.minikube\machines\minikube\id_rsa...
I0502 10:17:41.673164   39300 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0502 10:17:41.858048   39300 machine.go:94] provisionDockerMachine start ...
I0502 10:17:41.866542   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0502 10:17:42.057611   39300 main.go:141] libmachine: Using SSH client type: native
I0502 10:17:42.064965   39300 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6ba1c0] 0x6bcda0 <nil>  [] 0s} 127.0.0.1 65403 <nil> <nil>}
I0502 10:17:42.064965   39300 main.go:141] libmachine: About to run SSH command:
hostname
I0502 10:17:42.199900   39300 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0502 10:17:42.199900   39300 ubuntu.go:169] provisioning hostname "minikube"
I0502 10:17:42.207068   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0502 10:17:42.394800   39300 main.go:141] libmachine: Using SSH client type: native
I0502 10:17:42.394800   39300 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6ba1c0] 0x6bcda0 <nil>  [] 0s} 127.0.0.1 65403 <nil> <nil>}
I0502 10:17:42.394800   39300 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0502 10:17:42.573365   39300 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0502 10:17:42.583065   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0502 10:17:42.727457   39300 main.go:141] libmachine: Using SSH client type: native
I0502 10:17:42.728174   39300 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6ba1c0] 0x6bcda0 <nil>  [] 0s} 127.0.0.1 65403 <nil> <nil>}
I0502 10:17:42.728174   39300 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0502 10:17:42.871005   39300 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0502 10:17:42.871064   39300 ubuntu.go:175] set auth options {CertDir:C:\Users\admin\.minikube CaCertPath:C:\Users\admin\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\admin\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\admin\.minikube\machines\server.pem ServerKeyPath:C:\Users\admin\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\admin\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\admin\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\admin\.minikube}
I0502 10:17:42.871064   39300 ubuntu.go:177] setting up certificates
I0502 10:17:42.871064   39300 provision.go:84] configureAuth start
I0502 10:17:42.879285   39300 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0502 10:17:43.036401   39300 provision.go:143] copyHostCerts
I0502 10:17:43.037009   39300 exec_runner.go:144] found C:\Users\admin\.minikube/ca.pem, removing ...
I0502 10:17:43.037009   39300 exec_runner.go:203] rm: C:\Users\admin\.minikube\ca.pem
I0502 10:17:43.037009   39300 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\ca.pem --> C:\Users\admin\.minikube/ca.pem (1074 bytes)
I0502 10:17:43.038052   39300 exec_runner.go:144] found C:\Users\admin\.minikube/cert.pem, removing ...
I0502 10:17:43.038052   39300 exec_runner.go:203] rm: C:\Users\admin\.minikube\cert.pem
I0502 10:17:43.038052   39300 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\cert.pem --> C:\Users\admin\.minikube/cert.pem (1119 bytes)
I0502 10:17:43.050699   39300 exec_runner.go:144] found C:\Users\admin\.minikube/key.pem, removing ...
I0502 10:17:43.050764   39300 exec_runner.go:203] rm: C:\Users\admin\.minikube\key.pem
I0502 10:17:43.050764   39300 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\key.pem --> C:\Users\admin\.minikube/key.pem (1679 bytes)
I0502 10:17:43.052069   39300 provision.go:117] generating server cert: C:\Users\admin\.minikube\machines\server.pem ca-key=C:\Users\admin\.minikube\certs\ca.pem private-key=C:\Users\admin\.minikube\certs\ca-key.pem org=admin.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0502 10:17:43.426722   39300 provision.go:177] copyRemoteCerts
I0502 10:17:43.437213   39300 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0502 10:17:43.444662   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0502 10:17:43.584893   39300 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65403 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0502 10:17:43.696621   39300 ssh_runner.go:362] scp C:\Users\admin\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0502 10:17:43.729815   39300 ssh_runner.go:362] scp C:\Users\admin\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0502 10:17:43.760537   39300 ssh_runner.go:362] scp C:\Users\admin\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0502 10:17:43.794635   39300 provision.go:87] duration metric: took 923.567ms to configureAuth
I0502 10:17:43.794635   39300 ubuntu.go:193] setting minikube options for container-runtime
I0502 10:17:43.795158   39300 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0502 10:17:43.801040   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0502 10:17:43.941077   39300 main.go:141] libmachine: Using SSH client type: native
I0502 10:17:43.941077   39300 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6ba1c0] 0x6bcda0 <nil>  [] 0s} 127.0.0.1 65403 <nil> <nil>}
I0502 10:17:43.941077   39300 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0502 10:17:44.091002   39300 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0502 10:17:44.091002   39300 ubuntu.go:71] root file system type: overlay
I0502 10:17:44.091208   39300 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0502 10:17:44.100471   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0502 10:17:44.249866   39300 main.go:141] libmachine: Using SSH client type: native
I0502 10:17:44.250395   39300 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6ba1c0] 0x6bcda0 <nil>  [] 0s} 127.0.0.1 65403 <nil> <nil>}
I0502 10:17:44.250395   39300 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0502 10:17:44.405925   39300 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0502 10:17:44.416511   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0502 10:17:44.609837   39300 main.go:141] libmachine: Using SSH client type: native
I0502 10:17:44.610357   39300 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6ba1c0] 0x6bcda0 <nil>  [] 0s} 127.0.0.1 65403 <nil> <nil>}
I0502 10:17:44.610357   39300 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0502 10:17:45.708038   39300 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-11 10:51:59.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-05-02 08:17:44.384251131 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0502 10:17:45.708038   39300 machine.go:97] duration metric: took 3.8499733s to provisionDockerMachine
I0502 10:17:45.708038   39300 client.go:171] duration metric: took 54.0486469s to LocalClient.Create
I0502 10:17:45.708038   39300 start.go:167] duration metric: took 54.0486469s to libmachine.API.Create "minikube"
I0502 10:17:45.708189   39300 start.go:293] postStartSetup for "minikube" (driver="docker")
I0502 10:17:45.708189   39300 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0502 10:17:45.718665   39300 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0502 10:17:45.725160   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0502 10:17:45.883591   39300 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65403 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0502 10:17:46.001007   39300 ssh_runner.go:195] Run: cat /etc/os-release
I0502 10:17:46.007385   39300 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0502 10:17:46.007385   39300 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0502 10:17:46.007385   39300 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0502 10:17:46.007385   39300 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0502 10:17:46.007385   39300 filesync.go:126] Scanning C:\Users\admin\.minikube\addons for local assets ...
I0502 10:17:46.007897   39300 filesync.go:126] Scanning C:\Users\admin\.minikube\files for local assets ...
I0502 10:17:46.007897   39300 start.go:296] duration metric: took 299.7066ms for postStartSetup
I0502 10:17:46.016992   39300 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0502 10:17:46.165215   39300 profile.go:143] Saving config to C:\Users\admin\.minikube\profiles\minikube\config.json ...
I0502 10:17:46.168475   39300 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0502 10:17:46.176938   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0502 10:17:46.356381   39300 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65403 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0502 10:17:46.468951   39300 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0502 10:17:46.482220   39300 start.go:128] duration metric: took 54.8249542s to createHost
I0502 10:17:46.482220   39300 start.go:83] releasing machines lock for "minikube", held for 54.8254824s
I0502 10:17:46.493518   39300 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0502 10:17:46.639787   39300 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0502 10:17:46.641112   39300 ssh_runner.go:195] Run: cat /version.json
I0502 10:17:46.650567   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0502 10:17:46.650894   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0502 10:17:46.893295   39300 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65403 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0502 10:17:46.901011   39300 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65403 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0502 10:17:47.135054   39300 ssh_runner.go:195] Run: systemctl --version
I0502 10:17:47.144675   39300 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0502 10:17:47.159525   39300 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0502 10:17:47.174195   39300 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0502 10:17:47.187369   39300 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0502 10:17:47.246491   39300 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0502 10:17:47.246491   39300 start.go:494] detecting cgroup driver to use...
I0502 10:17:47.246491   39300 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0502 10:17:47.246613   39300 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0502 10:17:47.276374   39300 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0502 10:17:47.295149   39300 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0502 10:17:47.310680   39300 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0502 10:17:47.312787   39300 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0502 10:17:47.334627   39300 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0502 10:17:47.355989   39300 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0502 10:17:47.384607   39300 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0502 10:17:47.411958   39300 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0502 10:17:47.431552   39300 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0502 10:17:47.451501   39300 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0502 10:17:47.469911   39300 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0502 10:17:47.498568   39300 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0502 10:17:47.522369   39300 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0502 10:17:47.550340   39300 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0502 10:17:47.689562   39300 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0502 10:17:47.821218   39300 start.go:494] detecting cgroup driver to use...
I0502 10:17:47.821218   39300 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0502 10:17:47.831654   39300 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0502 10:17:47.854412   39300 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0502 10:17:47.868711   39300 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0502 10:17:47.887130   39300 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0502 10:17:47.915000   39300 ssh_runner.go:195] Run: which cri-dockerd
I0502 10:17:47.933375   39300 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0502 10:17:47.951917   39300 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0502 10:17:47.995782   39300 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0502 10:17:48.135713   39300 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0502 10:17:48.310976   39300 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0502 10:17:48.310976   39300 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0502 10:17:48.340373   39300 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0502 10:17:48.612559   39300 ssh_runner.go:195] Run: sudo systemctl restart docker
I0502 10:17:49.129490   39300 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0502 10:17:49.155741   39300 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0502 10:17:49.185513   39300 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0502 10:17:49.357226   39300 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0502 10:17:49.528834   39300 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0502 10:17:49.667991   39300 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0502 10:17:49.700985   39300 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0502 10:17:49.742912   39300 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0502 10:17:49.887149   39300 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0502 10:17:49.980056   39300 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0502 10:17:49.982504   39300 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0502 10:17:49.992310   39300 start.go:562] Will wait 60s for crictl version
I0502 10:17:49.995263   39300 ssh_runner.go:195] Run: which crictl
I0502 10:17:50.018273   39300 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0502 10:17:50.072092   39300 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0502 10:17:50.080497   39300 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0502 10:17:50.121732   39300 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0502 10:17:50.153064   39300 out.go:204] 🐳  Preparing Kubernetes v1.30.0 on Docker 26.0.1 ...
I0502 10:17:50.160180   39300 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0502 10:17:50.475653   39300 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0502 10:17:50.477247   39300 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0502 10:17:50.483563   39300 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0502 10:17:50.504974   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0502 10:17:50.659742   39300 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0502 10:17:50.659742   39300 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0502 10:17:50.665158   39300 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0502 10:17:50.692300   39300 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0502 10:17:50.692300   39300 docker.go:615] Images already preloaded, skipping extraction
I0502 10:17:50.704299   39300 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0502 10:17:50.742220   39300 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0502 10:17:50.742220   39300 cache_images.go:84] Images are preloaded, skipping loading
I0502 10:17:50.742220   39300 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0502 10:17:50.742220   39300 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0502 10:17:50.752060   39300 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0502 10:17:50.837820   39300 cni.go:84] Creating CNI manager for ""
I0502 10:17:50.838325   39300 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0502 10:17:50.838325   39300 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0502 10:17:50.838325   39300 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0502 10:17:50.838325   39300 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0502 10:17:50.849442   39300 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0502 10:17:50.875174   39300 binaries.go:44] Found k8s binaries, skipping transfer
I0502 10:17:50.886566   39300 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0502 10:17:50.902599   39300 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0502 10:17:50.934536   39300 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0502 10:17:50.961988   39300 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0502 10:17:50.988596   39300 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0502 10:17:50.994258   39300 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0502 10:17:51.021752   39300 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0502 10:17:51.212310   39300 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0502 10:17:51.255968   39300 certs.go:68] Setting up C:\Users\admin\.minikube\profiles\minikube for IP: 192.168.49.2
I0502 10:17:51.255968   39300 certs.go:194] generating shared ca certs ...
I0502 10:17:51.255968   39300 certs.go:226] acquiring lock for ca certs: {Name:mkfd7a320f78a704b321a6e757eb4483941c4372 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0502 10:17:51.281378   39300 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\admin\.minikube\ca.key
I0502 10:17:51.304201   39300 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\admin\.minikube\proxy-client-ca.key
I0502 10:17:51.305337   39300 certs.go:256] generating profile certs ...
I0502 10:17:51.305852   39300 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\admin\.minikube\profiles\minikube\client.key
I0502 10:17:51.305852   39300 crypto.go:68] Generating cert C:\Users\admin\.minikube\profiles\minikube\client.crt with IP's: []
I0502 10:17:51.413231   39300 crypto.go:156] Writing cert to C:\Users\admin\.minikube\profiles\minikube\client.crt ...
I0502 10:17:51.413231   39300 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\client.crt: {Name:mk1c354620d23d1db4d5f75cf7680da6ff84fa10 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0502 10:17:51.414236   39300 crypto.go:164] Writing key to C:\Users\admin\.minikube\profiles\minikube\client.key ...
I0502 10:17:51.414236   39300 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\client.key: {Name:mk6fd8ed109c6eb3817685a7b3c3510425de957a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0502 10:17:51.414236   39300 certs.go:363] generating signed profile cert for "minikube": C:\Users\admin\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0502 10:17:51.414236   39300 crypto.go:68] Generating cert C:\Users\admin\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0502 10:17:51.634564   39300 crypto.go:156] Writing cert to C:\Users\admin\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0502 10:17:51.634564   39300 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mk024d76e8c59ac79cd6bb75ac144b6e3da003e7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0502 10:17:51.634564   39300 crypto.go:164] Writing key to C:\Users\admin\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0502 10:17:51.634564   39300 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mkebe36927a54701b6bfc7aa3fc834fd80d0bb4b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0502 10:17:51.635579   39300 certs.go:381] copying C:\Users\admin\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\admin\.minikube\profiles\minikube\apiserver.crt
I0502 10:17:51.648517   39300 certs.go:385] copying C:\Users\admin\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\admin\.minikube\profiles\minikube\apiserver.key
I0502 10:17:51.648517   39300 certs.go:363] generating signed profile cert for "aggregator": C:\Users\admin\.minikube\profiles\minikube\proxy-client.key
I0502 10:17:51.648517   39300 crypto.go:68] Generating cert C:\Users\admin\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0502 10:17:51.850275   39300 crypto.go:156] Writing cert to C:\Users\admin\.minikube\profiles\minikube\proxy-client.crt ...
I0502 10:17:51.850275   39300 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\proxy-client.crt: {Name:mk95af3ec4c02cfa13d7d53e2939b9964b736a77 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0502 10:17:51.850275   39300 crypto.go:164] Writing key to C:\Users\admin\.minikube\profiles\minikube\proxy-client.key ...
I0502 10:17:51.850275   39300 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\proxy-client.key: {Name:mkda28e207fc620d768ca92b69e7df85c52a9e02 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0502 10:17:51.860343   39300 certs.go:484] found cert: C:\Users\admin\.minikube\certs\ca-key.pem (1679 bytes)
I0502 10:17:51.860343   39300 certs.go:484] found cert: C:\Users\admin\.minikube\certs\ca.pem (1074 bytes)
I0502 10:17:51.860343   39300 certs.go:484] found cert: C:\Users\admin\.minikube\certs\cert.pem (1119 bytes)
I0502 10:17:51.862959   39300 certs.go:484] found cert: C:\Users\admin\.minikube\certs\key.pem (1679 bytes)
I0502 10:17:51.864928   39300 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0502 10:17:51.895893   39300 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0502 10:17:51.923800   39300 ssh_runner.go:362] scp C:\Users\admin\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0502 10:17:51.955827   39300 ssh_runner.go:362] scp C:\Users\admin\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0502 10:17:51.988750   39300 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0502 10:17:52.018450   39300 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0502 10:17:52.053822   39300 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0502 10:17:52.083011   39300 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0502 10:17:52.112193   39300 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0502 10:17:52.144109   39300 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0502 10:17:52.173008   39300 ssh_runner.go:195] Run: openssl version
I0502 10:17:52.206843   39300 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0502 10:17:52.226215   39300 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0502 10:17:52.233109   39300 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Apr 29 09:32 /usr/share/ca-certificates/minikubeCA.pem
I0502 10:17:52.234729   39300 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0502 10:17:52.251533   39300 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0502 10:17:52.269291   39300 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0502 10:17:52.275221   39300 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0502 10:17:52.275221   39300 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0502 10:17:52.281284   39300 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0502 10:17:52.309618   39300 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0502 10:17:52.329327   39300 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0502 10:17:52.341840   39300 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I0502 10:17:52.350678   39300 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0502 10:17:52.362831   39300 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0502 10:17:52.362831   39300 kubeadm.go:156] found existing configuration files:

I0502 10:17:52.371049   39300 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0502 10:17:52.383710   39300 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0502 10:17:52.390872   39300 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0502 10:17:52.410340   39300 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0502 10:17:52.422663   39300 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0502 10:17:52.432903   39300 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0502 10:17:52.454511   39300 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0502 10:17:52.467180   39300 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0502 10:17:52.474536   39300 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0502 10:17:52.494760   39300 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0502 10:17:52.506776   39300 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0502 10:17:52.514401   39300 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0502 10:17:52.546174   39300 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0502 10:17:52.680607   39300 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I0502 10:17:52.680607   39300 kubeadm.go:309] [preflight] Running pre-flight checks
I0502 10:17:52.928922   39300 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I0502 10:17:52.928922   39300 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0502 10:17:52.928922   39300 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0502 10:17:53.754332   39300 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0502 10:17:53.778649   39300 out.go:204]     ▪ Generating certificates and keys ...
I0502 10:17:53.914003   39300 kubeadm.go:309] [certs] Using existing ca certificate authority
I0502 10:17:53.914003   39300 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I0502 10:17:54.327107   39300 kubeadm.go:309] [certs] Generating "apiserver-kubelet-client" certificate and key
I0502 10:17:54.420127   39300 kubeadm.go:309] [certs] Generating "front-proxy-ca" certificate and key
I0502 10:17:54.816132   39300 kubeadm.go:309] [certs] Generating "front-proxy-client" certificate and key
I0502 10:17:55.161383   39300 kubeadm.go:309] [certs] Generating "etcd/ca" certificate and key
I0502 10:17:55.469070   39300 kubeadm.go:309] [certs] Generating "etcd/server" certificate and key
I0502 10:17:55.469581   39300 kubeadm.go:309] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0502 10:17:55.538478   39300 kubeadm.go:309] [certs] Generating "etcd/peer" certificate and key
I0502 10:17:55.538478   39300 kubeadm.go:309] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0502 10:17:55.626906   39300 kubeadm.go:309] [certs] Generating "etcd/healthcheck-client" certificate and key
I0502 10:17:56.009981   39300 kubeadm.go:309] [certs] Generating "apiserver-etcd-client" certificate and key
I0502 10:17:56.220596   39300 kubeadm.go:309] [certs] Generating "sa" key and public key
I0502 10:17:56.221110   39300 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0502 10:17:56.430968   39300 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I0502 10:17:56.530895   39300 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0502 10:17:56.709554   39300 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0502 10:17:56.888981   39300 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0502 10:17:57.024404   39300 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0502 10:17:57.026041   39300 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0502 10:17:57.031020   39300 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0502 10:17:57.034564   39300 out.go:204]     ▪ Booting up control plane ...
I0502 10:17:57.035079   39300 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0502 10:17:57.035079   39300 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0502 10:17:57.035599   39300 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0502 10:17:57.054731   39300 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0502 10:17:57.057063   39300 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0502 10:17:57.057063   39300 kubeadm.go:309] [kubelet-start] Starting the kubelet
I0502 10:17:57.205117   39300 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0502 10:17:57.205117   39300 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I0502 10:17:57.706761   39300 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 501.788872ms
I0502 10:17:57.706761   39300 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0502 10:18:05.709297   39300 kubeadm.go:309] [api-check] The API server is healthy after 8.003152267s
I0502 10:18:05.723011   39300 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0502 10:18:05.748256   39300 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0502 10:18:05.779941   39300 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I0502 10:18:05.780532   39300 kubeadm.go:309] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0502 10:18:05.795629   39300 kubeadm.go:309] [bootstrap-token] Using token: 9fur8e.k4ic7h4n0n1b2obh
I0502 10:18:05.798300   39300 out.go:204]     ▪ Configuring RBAC rules ...
I0502 10:18:05.799793   39300 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0502 10:18:05.818621   39300 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0502 10:18:05.858880   39300 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0502 10:18:05.873109   39300 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0502 10:18:05.880039   39300 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0502 10:18:05.887878   39300 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0502 10:18:06.119977   39300 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0502 10:18:06.603052   39300 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I0502 10:18:07.129934   39300 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I0502 10:18:07.169822   39300 kubeadm.go:309] 
I0502 10:18:07.169822   39300 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I0502 10:18:07.169822   39300 kubeadm.go:309] 
I0502 10:18:07.169822   39300 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I0502 10:18:07.169822   39300 kubeadm.go:309] 
I0502 10:18:07.169822   39300 kubeadm.go:309]   mkdir -p $HOME/.kube
I0502 10:18:07.169822   39300 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0502 10:18:07.170807   39300 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0502 10:18:07.170807   39300 kubeadm.go:309] 
I0502 10:18:07.170807   39300 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I0502 10:18:07.170807   39300 kubeadm.go:309] 
I0502 10:18:07.170807   39300 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0502 10:18:07.170807   39300 kubeadm.go:309] 
I0502 10:18:07.171317   39300 kubeadm.go:309] You should now deploy a pod network to the cluster.
I0502 10:18:07.171740   39300 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0502 10:18:07.171740   39300 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0502 10:18:07.171740   39300 kubeadm.go:309] 
I0502 10:18:07.172248   39300 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I0502 10:18:07.172248   39300 kubeadm.go:309] and service account keys on each node and then running the following as root:
I0502 10:18:07.172248   39300 kubeadm.go:309] 
I0502 10:18:07.172248   39300 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token 9fur8e.k4ic7h4n0n1b2obh \
I0502 10:18:07.172694   39300 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:614893898cf289fdd4b0b92f98e4838363ad0c21d1d91d43e355be39cbd93a45 \
I0502 10:18:07.172694   39300 kubeadm.go:309] 	--control-plane 
I0502 10:18:07.172694   39300 kubeadm.go:309] 
I0502 10:18:07.172694   39300 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I0502 10:18:07.172694   39300 kubeadm.go:309] 
I0502 10:18:07.173448   39300 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token 9fur8e.k4ic7h4n0n1b2obh \
I0502 10:18:07.173953   39300 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:614893898cf289fdd4b0b92f98e4838363ad0c21d1d91d43e355be39cbd93a45 
I0502 10:18:07.174120   39300 kubeadm.go:309] 	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I0502 10:18:07.174120   39300 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0502 10:18:07.174120   39300 cni.go:84] Creating CNI manager for ""
I0502 10:18:07.174120   39300 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0502 10:18:07.176399   39300 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0502 10:18:07.191265   39300 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0502 10:18:07.221090   39300 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0502 10:18:07.267217   39300 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0502 10:18:07.286726   39300 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_05_02T10_18_07_0700 minikube.k8s.io/version=v1.33.0 minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0502 10:18:07.292134   39300 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0502 10:18:07.304518   39300 ops.go:34] apiserver oom_adj: -16
I0502 10:18:07.511475   39300 kubeadm.go:1107] duration metric: took 244.2563ms to wait for elevateKubeSystemPrivileges
W0502 10:18:07.511475   39300 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I0502 10:18:07.511475   39300 kubeadm.go:393] duration metric: took 15.2361893s to StartCluster
I0502 10:18:07.511475   39300 settings.go:142] acquiring lock: {Name:mk5c419c3fabff3b09a504acb297c33a24a93937 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0502 10:18:07.511982   39300 settings.go:150] Updating kubeconfig:  C:\Users\admin\.kube\config
I0502 10:18:07.514978   39300 lock.go:35] WriteFile acquiring C:\Users\admin\.kube\config: {Name:mk0f6f650be7ab5b3e1ae83f14563cf1eb0bd30b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0502 10:18:07.517874   39300 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0502 10:18:07.517874   39300 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0502 10:18:07.523658   39300 out.go:177] 🔎  Verifying Kubernetes components...
I0502 10:18:07.517874   39300 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0502 10:18:07.519458   39300 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0502 10:18:07.524166   39300 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0502 10:18:07.524166   39300 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0502 10:18:07.525897   39300 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0502 10:18:07.528753   39300 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0502 10:18:07.527453   39300 host.go:66] Checking if "minikube" exists ...
I0502 10:18:07.550451   39300 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0502 10:18:07.564615   39300 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0502 10:18:07.584325   39300 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0502 10:18:07.871898   39300 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0502 10:18:07.874122   39300 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0502 10:18:07.874122   39300 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0502 10:18:07.883302   39300 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0502 10:18:07.883302   39300 host.go:66] Checking if "minikube" exists ...
I0502 10:18:07.891781   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0502 10:18:07.922446   39300 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0502 10:18:08.009472   39300 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0502 10:18:08.203608   39300 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0502 10:18:08.278520   39300 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0502 10:18:08.278520   39300 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0502 10:18:08.287809   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0502 10:18:08.289487   39300 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65403 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0502 10:18:08.585209   39300 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65403 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0502 10:18:09.010237   39300 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0502 10:18:09.225769   39300 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0502 10:18:09.235771   39300 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (1.226294s)
I0502 10:18:09.235771   39300 start.go:946] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0502 10:18:09.237294   39300 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (1.0336809s)
I0502 10:18:09.247377   39300 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0502 10:18:09.788830   39300 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0502 10:18:10.018414   39300 api_server.go:52] waiting for apiserver process to appear ...
I0502 10:18:10.038433   39300 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0502 10:18:10.364645   39300 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.1388716s)
I0502 10:18:10.365171   39300 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.3549285s)
I0502 10:18:10.365171   39300 api_server.go:72] duration metric: took 2.8472849s to wait for apiserver process to appear ...
I0502 10:18:10.365171   39300 api_server.go:88] waiting for apiserver healthz status ...
I0502 10:18:10.365171   39300 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65402/healthz ...
I0502 10:18:10.386029   39300 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0502 10:18:10.387187   39300 addons.go:505] duration metric: took 2.8693004s for enable addons: enabled=[storage-provisioner default-storageclass]
I0502 10:18:10.388762   39300 api_server.go:279] https://127.0.0.1:65402/healthz returned 200:
ok
I0502 10:18:10.391981   39300 api_server.go:141] control plane version: v1.30.0
I0502 10:18:10.391981   39300 api_server.go:131] duration metric: took 26.8097ms to wait for apiserver health ...
I0502 10:18:10.391981   39300 system_pods.go:43] waiting for kube-system pods to appear ...
I0502 10:18:10.417699   39300 system_pods.go:59] 5 kube-system pods found
I0502 10:18:10.417699   39300 system_pods.go:61] "etcd-minikube" [0bb288ce-49d7-4224-a6fe-8c68fe907b65] Running
I0502 10:18:10.417699   39300 system_pods.go:61] "kube-apiserver-minikube" [642924be-e6d8-46ce-a508-44a2877165fd] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0502 10:18:10.417699   39300 system_pods.go:61] "kube-controller-manager-minikube" [c294dedf-e1b0-4710-8ac3-e15a06defd66] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0502 10:18:10.417699   39300 system_pods.go:61] "kube-scheduler-minikube" [27cbf3d2-84b9-440e-974c-8ae3f8a7fcb3] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0502 10:18:10.417699   39300 system_pods.go:61] "storage-provisioner" [ccab42d0-cc3d-40a0-8eca-af093f672b3a] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0502 10:18:10.417699   39300 system_pods.go:74] duration metric: took 25.718ms to wait for pod list to return data ...
I0502 10:18:10.417699   39300 kubeadm.go:576] duration metric: took 2.8998126s to wait for: map[apiserver:true system_pods:true]
I0502 10:18:10.417699   39300 node_conditions.go:102] verifying NodePressure condition ...
I0502 10:18:10.427129   39300 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0502 10:18:10.427129   39300 node_conditions.go:123] node cpu capacity is 4
I0502 10:18:10.427129   39300 node_conditions.go:105] duration metric: took 9.4304ms to run NodePressure ...
I0502 10:18:10.427129   39300 start.go:240] waiting for startup goroutines ...
I0502 10:18:10.427129   39300 start.go:245] waiting for cluster config update ...
I0502 10:18:10.427129   39300 start.go:254] writing updated cluster config ...
I0502 10:18:10.434197   39300 ssh_runner.go:195] Run: rm -f paused
I0502 10:18:10.654253   39300 start.go:600] kubectl: 1.29.2, cluster: 1.30.0 (minor skew: 1)
I0502 10:18:10.655379   39300 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 04 23:33:57 minikube dockerd[1195]: time="2024-05-04T23:33:57.898363331Z" level=info msg="ignoring event" container=4357b045d28e54fa0c7ea41daaac3774023f30e9d2945f08678e6cdcb63669a6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:36:20 minikube dockerd[1195]: time="2024-05-04T23:36:20.342209056Z" level=info msg="ignoring event" container=26ce907b60304b1e139ca1ca46109ddaddc58f203dc716cc10238ac72db1fa95 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:37:55 minikube dockerd[1195]: time="2024-05-04T23:37:55.368273838Z" level=info msg="ignoring event" container=9ad8440d7c888b1b30d8e8db538fabde451fcd3923c4c3019cd7e544011a7a4f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:37:55 minikube dockerd[1195]: time="2024-05-04T23:37:55.428887272Z" level=info msg="ignoring event" container=0d5e854af2e4625de610ae5e06824f78f39c8e8dbdb30dc32480c91f696fd0d3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:37:55 minikube dockerd[1195]: time="2024-05-04T23:37:55.664071042Z" level=info msg="ignoring event" container=f9943a8578710c2b1021bacc703fc4502aa5864ff9266810d64a8f7d651a2358 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:37:55 minikube dockerd[1195]: time="2024-05-04T23:37:55.744603273Z" level=info msg="ignoring event" container=00f2aa7e6b6760a6e40c1e6bf7d36f54c6314aa0e9f9c0ce37495825e8116265 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:39:27 minikube dockerd[1195]: time="2024-05-04T23:39:27.551183951Z" level=info msg="ignoring event" container=eebe6622e8fb2016e7655d0ea0b74960334ee42fe7cf603c0bec5c8e5057d73f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:41:16 minikube cri-dockerd[1417]: time="2024-05-04T23:41:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2e252ffca91d31f009dc123f037a38d764f1d79f380fb3ce9e035c2448436cde/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 04 23:41:16 minikube cri-dockerd[1417]: time="2024-05-04T23:41:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/053ead46899a92c88ada03032b3885ac00d27729aa7f966c6fce5b86b1b78a73/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 04 23:41:17 minikube cri-dockerd[1417]: time="2024-05-04T23:41:17Z" level=info msg="Stop pulling image wordpress:latest: Status: Image is up to date for wordpress:latest"
May 04 23:41:18 minikube cri-dockerd[1417]: time="2024-05-04T23:41:18Z" level=info msg="Stop pulling image wordpress:latest: Status: Image is up to date for wordpress:latest"
May 04 23:43:54 minikube dockerd[1195]: time="2024-05-04T23:43:54.905625344Z" level=info msg="ignoring event" container=951bc693103465cb2ff0edcada78cefd235e4be43dbe7e0b421b027482183636 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:47:26 minikube dockerd[1195]: time="2024-05-04T23:47:26.653244548Z" level=info msg="ignoring event" container=c2069ca11f819bfb08236a96b665834fab59550f5e3397275c8614605aea0837 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:47:27 minikube dockerd[1195]: time="2024-05-04T23:47:27.045333832Z" level=info msg="ignoring event" container=a5675c2c2cee33100ff8de0194d171d3dbb0d0999eaa59a07a191d54a449e719 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:47:27 minikube dockerd[1195]: time="2024-05-04T23:47:27.303204012Z" level=info msg="ignoring event" container=b3b409689a62cff7fb8893c197858dab560a186ae4c3ed6d6c2a24f2a9fb31b3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:47:27 minikube dockerd[1195]: time="2024-05-04T23:47:27.539504990Z" level=info msg="ignoring event" container=2e252ffca91d31f009dc123f037a38d764f1d79f380fb3ce9e035c2448436cde module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:47:27 minikube dockerd[1195]: time="2024-05-04T23:47:27.628589969Z" level=info msg="ignoring event" container=053ead46899a92c88ada03032b3885ac00d27729aa7f966c6fce5b86b1b78a73 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:47:27 minikube dockerd[1195]: time="2024-05-04T23:47:27.896524944Z" level=info msg="ignoring event" container=84b125f89bc5bb5643a6061b029e09b038180cc7b71496bb879d1115f863b09c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:47:28 minikube dockerd[1195]: time="2024-05-04T23:47:28.199960624Z" level=info msg="ignoring event" container=175a91a7fb088a5b1c856a64b013e3d573c94f256bc8dc2d1dfd4da9599eff80 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:48:15 minikube cri-dockerd[1417]: time="2024-05-04T23:48:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f8e2ad019da2466b85b496a58c0945dddc8d869733bc36a8a0b35a8ebd2a8c33/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 04 23:48:16 minikube cri-dockerd[1417]: time="2024-05-04T23:48:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5a8101788e43a02eefb59babd3d988722ae36b96ed2bcc912e935de252241950/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 04 23:48:17 minikube cri-dockerd[1417]: time="2024-05-04T23:48:17Z" level=info msg="Stop pulling image wordpress:latest: Status: Image is up to date for wordpress:latest"
May 04 23:49:04 minikube cri-dockerd[1417]: time="2024-05-04T23:49:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/563445a3d5dbd81e1ce92fe5e176608ad89aca87eae42a5f3a7432497dba898f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 04 23:49:05 minikube cri-dockerd[1417]: time="2024-05-04T23:49:05Z" level=info msg="Stop pulling image wordpress:latest: Status: Image is up to date for wordpress:latest"
May 04 23:49:06 minikube dockerd[1195]: time="2024-05-04T23:49:06.685150658Z" level=info msg="ignoring event" container=50b7f69ed6c3935294e70dbae845e5bb6d91c85757960d67d91c56097473d6f6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 04 23:49:07 minikube dockerd[1195]: time="2024-05-04T23:49:07.058534229Z" level=info msg="ignoring event" container=5a8101788e43a02eefb59babd3d988722ae36b96ed2bcc912e935de252241950 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:06:28 minikube dockerd[1195]: time="2024-05-05T00:06:28.189555548Z" level=warning msg="failed to close stdin: process does not exist 4f1eade5f740b431a3042a6793e7dac2a7796c6744766281dc7fd242cf18f739: not found"
May 05 00:08:31 minikube cri-dockerd[1417]: time="2024-05-05T00:08:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/56901ec00205913aca022037f3f2ea313a3662301f4428c5cfc6754e02262f9e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 05 00:08:33 minikube cri-dockerd[1417]: time="2024-05-05T00:08:33Z" level=info msg="Stop pulling image wordpress:latest: Status: Image is up to date for wordpress:latest"
May 05 00:08:35 minikube dockerd[1195]: time="2024-05-05T00:08:35.796645783Z" level=info msg="ignoring event" container=14e96fc4dd9a3c6ef5d0f5f7dd17c75d3fb2f5035a9e1465d15546ded9d4f9e6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:08:36 minikube dockerd[1195]: time="2024-05-05T00:08:36.082502731Z" level=info msg="ignoring event" container=563445a3d5dbd81e1ce92fe5e176608ad89aca87eae42a5f3a7432497dba898f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:09:57 minikube cri-dockerd[1417]: time="2024-05-05T00:09:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5dafd6468c2af2e46f435b368c7ded192c5ffc925db0aef71d168e50270a1534/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 05 00:09:59 minikube cri-dockerd[1417]: time="2024-05-05T00:09:59Z" level=info msg="Stop pulling image wordpress:latest: Status: Image is up to date for wordpress:latest"
May 05 00:10:00 minikube dockerd[1195]: time="2024-05-05T00:10:00.840389474Z" level=info msg="ignoring event" container=df36210a708f3ba101984cceb9b3f7d4dd217bbfb922cc8212c705e7842fd92b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:10:01 minikube dockerd[1195]: time="2024-05-05T00:10:01.056935851Z" level=info msg="ignoring event" container=56901ec00205913aca022037f3f2ea313a3662301f4428c5cfc6754e02262f9e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:14:19 minikube dockerd[1195]: time="2024-05-05T00:14:19.879614213Z" level=info msg="ignoring event" container=e61073a337d03905cdd9917199fe60f00797e19e71a8bfb158ae9865a06fa584 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:14:20 minikube cri-dockerd[1417]: time="2024-05-05T00:14:20Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"wordpress-deployment-669654b585-trvzx_default\": unexpected command output nsenter: cannot open /proc/926900/ns/net: No such file or directory\n with error: exit status 1"
May 05 00:14:20 minikube dockerd[1195]: time="2024-05-05T00:14:20.159285841Z" level=info msg="ignoring event" container=5dafd6468c2af2e46f435b368c7ded192c5ffc925db0aef71d168e50270a1534 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:14:20 minikube dockerd[1195]: time="2024-05-05T00:14:20.695881826Z" level=info msg="ignoring event" container=77dc1251bab8ff25e6c3a6aa584e16ac3debf7d9282124246f9dbfe5cf2eb51d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:14:20 minikube dockerd[1195]: time="2024-05-05T00:14:20.899467529Z" level=info msg="ignoring event" container=f8e2ad019da2466b85b496a58c0945dddc8d869733bc36a8a0b35a8ebd2a8c33 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:16:06 minikube cri-dockerd[1417]: time="2024-05-05T00:16:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/452bbd95404b06a49cbbbe459f61a34c092186eaac4e5182de386c83ec8f1d4f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 05 00:16:08 minikube cri-dockerd[1417]: time="2024-05-05T00:16:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d582807dfb1ad9e85e874836e536434c5b1f776d48dc477cef3a90bd884ede94/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 05 00:16:20 minikube cri-dockerd[1417]: time="2024-05-05T00:16:20Z" level=info msg="Pulling image wordpress:6.2.1-apache: 5908153120ba: Downloading [==================>                                ]  34.42MB/91.64MB"
May 05 00:16:30 minikube cri-dockerd[1417]: time="2024-05-05T00:16:30Z" level=info msg="Pulling image wordpress:6.2.1-apache: 5908153120ba: Downloading [=================================================> ]  90.35MB/91.64MB"
May 05 00:16:40 minikube cri-dockerd[1417]: time="2024-05-05T00:16:40Z" level=info msg="Pulling image wordpress:6.2.1-apache: 5908153120ba: Extracting [==================================>                ]  64.06MB/91.64MB"
May 05 00:16:50 minikube cri-dockerd[1417]: time="2024-05-05T00:16:50Z" level=info msg="Pulling image wordpress:6.2.1-apache: f06b38c16403: Extracting [=============>                                     ]  5.308MB/18.99MB"
May 05 00:16:55 minikube cri-dockerd[1417]: time="2024-05-05T00:16:55Z" level=info msg="Stop pulling image wordpress:6.2.1-apache: Status: Downloaded newer image for wordpress:6.2.1-apache"
May 05 00:33:53 minikube dockerd[1195]: time="2024-05-05T00:33:53.900376514Z" level=info msg="ignoring event" container=31fc1302f4560729b9bacd999f17c7d0deeb1e8f2648ac1bf5dcb59a236b9641 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:33:54 minikube dockerd[1195]: time="2024-05-05T00:33:54.003803272Z" level=info msg="ignoring event" container=ea80fd102613c316085b04205b8170076bdfccd2994637d30e5f26790eff45e5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:33:54 minikube cri-dockerd[1417]: time="2024-05-05T00:33:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"wordpress-deployment-5cf59d7c46-m9hpm_default\": unexpected command output nsenter: cannot open /proc/928830/ns/net: No such file or directory\n with error: exit status 1"
May 05 00:33:54 minikube dockerd[1195]: time="2024-05-05T00:33:54.167969636Z" level=info msg="ignoring event" container=d582807dfb1ad9e85e874836e536434c5b1f776d48dc477cef3a90bd884ede94 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:33:54 minikube dockerd[1195]: time="2024-05-05T00:33:54.198885267Z" level=info msg="ignoring event" container=452bbd95404b06a49cbbbe459f61a34c092186eaac4e5182de386c83ec8f1d4f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:43:23 minikube cri-dockerd[1417]: time="2024-05-05T00:43:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8854ddc02bd6eddadef91aa08c7bc4f6c7d2434082066e25c619cab4294d14ec/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 05 00:43:34 minikube cri-dockerd[1417]: time="2024-05-05T00:43:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b9b0ba54c372c0c020d7e3312ff12f9f9ad13d9e63fe0c473eb91d5d3c491a25/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 05 00:46:24 minikube dockerd[1195]: time="2024-05-05T00:46:24.442992788Z" level=info msg="ignoring event" container=0b81b7525631b4183dcdacb15f8fdeed42b57f4eeb244e70892b7005798f94b5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:46:24 minikube dockerd[1195]: time="2024-05-05T00:46:24.731817514Z" level=info msg="ignoring event" container=291eade9315345cd07a38c9207fa1794bf6773666db5c9975acde8e2bf2d8c85 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:46:24 minikube dockerd[1195]: time="2024-05-05T00:46:24.763251949Z" level=info msg="ignoring event" container=8854ddc02bd6eddadef91aa08c7bc4f6c7d2434082066e25c619cab4294d14ec module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:46:25 minikube dockerd[1195]: time="2024-05-05T00:46:25.131751221Z" level=info msg="ignoring event" container=b9b0ba54c372c0c020d7e3312ff12f9f9ad13d9e63fe0c473eb91d5d3c491a25 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 05 00:46:33 minikube cri-dockerd[1417]: time="2024-05-05T00:46:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5d5c2606b5da63d175fea31ef6c901961fc241b67547ea49be02c9bf33a37954/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 05 00:47:49 minikube cri-dockerd[1417]: time="2024-05-05T00:47:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aacfd9bbfb003dd1817a5956b63a5a7f1847ba46af7794f454af7f28ffbd9ffa/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
b20cde7e162f1       b8ee07adfa917                                                                                                                2 minutes ago       Running             wordpress                 0                   aacfd9bbfb003       wordpress-deployment-6777f48f9b-fz8p9
a909ef711b006       42a3e526fca25                                                                                                                3 minutes ago       Running             mysql                     0                   5d5c2606b5da6       mysql-deployment-77fb8bc69c-w4mqc
69226188fd1c9       6e38f40d628db                                                                                                                39 hours ago        Running             storage-provisioner       3                   43b6df326659e       storage-provisioner
217b2fcaf9a2c       registry.k8s.io/ingress-nginx/controller@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c             2 days ago          Running             controller                0                   143926b8bd25d       ingress-nginx-controller-84df5799c-dptd4
279f04a579151       b29d748098e32                                                                                                                2 days ago          Exited              patch                     1                   6557d86e39af5       ingress-nginx-admission-patch-6gnlc
71b27e38fa894       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334   2 days ago          Exited              create                    0                   93168fd7c4ceb       ingress-nginx-admission-create-lgvrq
12df540ec5f6f       6e38f40d628db                                                                                                                2 days ago          Exited              storage-provisioner       2                   43b6df326659e       storage-provisioner
396b964817712       cbb01a7bd410d                                                                                                                2 days ago          Running             coredns                   0                   39aa0d3d9bd1c       coredns-7db6d8ff4d-gx5kr
da40952e26027       a0bf559e280cf                                                                                                                2 days ago          Running             kube-proxy                0                   06bfe1f463a00       kube-proxy-rrg4r
93f676895d83e       c42f13656d0b2                                                                                                                2 days ago          Running             kube-apiserver            0                   9e49a62173069       kube-apiserver-minikube
ed9b3255c75c3       3861cfcd7c04c                                                                                                                2 days ago          Running             etcd                      0                   fb6c336f110ad       etcd-minikube
4eae87c45fb28       c7aad43836fa5                                                                                                                2 days ago          Running             kube-controller-manager   0                   4b60d4862ccd4       kube-controller-manager-minikube
fa96e01ce6fd5       259c8277fcbbc                                                                                                                2 days ago          Running             kube-scheduler            0                   cf5e8e5c55ed3       kube-scheduler-minikube


==> controller_ingress [217b2fcaf9a2] <==
W0504 23:47:28.843924       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0504 23:47:28.844007       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0504 23:48:12.497019       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0504 23:48:12.497386       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0504 23:48:15.838361       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0504 23:48:15.838421       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0504 23:48:19.163816       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0504 23:48:19.163890       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0504 23:48:22.497657       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0504 23:48:22.497729       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0504 23:49:05.584033       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0504 23:49:05.584174       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0504 23:49:08.916828       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0504 23:49:08.916918       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0504 23:49:12.250391       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0504 23:49:12.250592       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:08:34.599471       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:08:34.599545       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:08:37.933743       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:08:37.933814       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:08:41.265681       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:08:41.265758       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:09:59.721854       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:09:59.721924       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:10:03.055875       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:10:03.055956       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:10:06.388873       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:10:06.388956       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:14:18.230803       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:14:18.230866       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:14:21.564506       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:14:21.564575       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:16:04.833109       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:16:04.833185       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:16:08.166745       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:16:08.166793       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:16:11.499748       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:16:11.501302       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:17:03.304547       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:17:03.304624       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:33:50.672494       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:33:50.672553       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:33:54.006077       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:33:54.006149       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:43:21.269807       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:43:21.269880       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:43:24.604136       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:43:24.604219       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:43:27.936688       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:43:27.936793       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:46:23.186113       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:46:23.186161       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:46:26.520073       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:46:26.520171       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:46:29.852838       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:46:29.852910       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:46:34.203998       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:46:34.204076       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store
W0505 00:47:45.488613       7 controller.go:1108] Error obtaining Endpoints for Service "default/app-service": no object matching key "default/app-service" in local store
W0505 00:47:45.488675       7 controller.go:1108] Error obtaining Endpoints for Service "default/app2-service": no object matching key "default/app2-service" in local store


==> coredns [396b96481771] <==
[INFO] 10.244.0.13:49159 - 21696 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000550259s
[INFO] 10.244.0.13:49159 - 31181 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000710176s
[INFO] 10.244.0.13:59309 - 61122 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000140814s
[INFO] 10.244.0.13:59309 - 51918 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000197221s
[INFO] 10.244.0.18:57153 - 31766 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.00040412s
[INFO] 10.244.0.18:57153 - 55579 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000491824s
[INFO] 10.244.0.18:49004 - 54002 "A IN mysql.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.00020381s
[INFO] 10.244.0.18:49004 - 19953 "AAAA IN mysql.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000134807s
[INFO] 10.244.0.18:54584 - 11285 "A IN mysql.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000145907s
[INFO] 10.244.0.18:54584 - 7699 "AAAA IN mysql.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000107506s
[INFO] 10.244.0.18:42681 - 25546 "A IN mysql. udp 23 false 512" - - 0 6.003606736s
[INFO] 10.244.0.18:42681 - 5835 "AAAA IN mysql. udp 23 false 512" - - 0 6.001744345s
[ERROR] plugin/errors: 2 mysql. AAAA: read udp 10.244.0.2:56610->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 mysql. A: read udp 10.244.0.2:40795->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.18:42681 - 25546 "A IN mysql. udp 23 false 512" - - 0 4.001582408s
[ERROR] plugin/errors: 2 mysql. A: read udp 10.244.0.2:38213->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.18:42681 - 5835 "AAAA IN mysql. udp 23 false 512" - - 0 4.016772975s
[ERROR] plugin/errors: 2 mysql. AAAA: read udp 10.244.0.2:42588->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.18:33389 - 63549 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000200911s
[INFO] 10.244.0.18:33389 - 27199 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000351119s
[INFO] 10.244.0.18:34757 - 60144 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000166008s
[INFO] 10.244.0.18:34757 - 61694 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.00020531s
[INFO] 10.244.0.18:38956 - 42875 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.00018891s
[INFO] 10.244.0.18:38956 - 33406 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000216111s
[INFO] 10.244.0.18:37510 - 17574 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000205511s
[INFO] 10.244.0.18:37510 - 8355 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000333517s
[INFO] 10.244.0.18:59359 - 27180 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000104305s
[INFO] 10.244.0.18:59359 - 18735 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000153608s
[INFO] 10.244.0.18:43611 - 14466 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.00018751s
[INFO] 10.244.0.18:43611 - 3712 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000288715s
[INFO] 10.244.0.18:39864 - 24581 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.00017671s
[INFO] 10.244.0.18:39864 - 9990 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000290515s
[INFO] 10.244.0.18:60015 - 34357 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.00019452s
[INFO] 10.244.0.18:60015 - 5940 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000312432s
[INFO] 10.244.0.18:43208 - 19446 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.00013486s
[INFO] 10.244.0.18:43208 - 30196 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000244428s
[INFO] 10.244.0.18:52108 - 24207 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.00018621s
[INFO] 10.244.0.18:52108 - 45440 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000292517s
[INFO] 10.244.0.18:38245 - 10187 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000140208s
[INFO] 10.244.0.18:38245 - 12745 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000203012s
[INFO] 10.244.0.18:41330 - 16086 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000127008s
[INFO] 10.244.0.18:41330 - 9432 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000228615s
[INFO] 10.244.0.18:37797 - 49888 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000148407s
[INFO] 10.244.0.18:37797 - 12771 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000208111s
[INFO] 10.244.0.18:38954 - 54847 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000798133s
[INFO] 10.244.0.18:38954 - 22844 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000751731s
[INFO] 10.244.0.18:39270 - 6038 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.003734187s
[INFO] 10.244.0.18:39270 - 60565 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.004452122s
[INFO] 10.244.0.18:42788 - 21889 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000160208s
[INFO] 10.244.0.18:42788 - 9091 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000244712s
[INFO] 10.244.0.18:56759 - 41416 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000322216s
[INFO] 10.244.0.18:56759 - 14549 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000484924s
[INFO] 10.244.0.18:38626 - 60533 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000214804s
[INFO] 10.244.0.18:38626 - 4468 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000295505s
[INFO] 10.244.0.18:45225 - 17811 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000150107s
[INFO] 10.244.0.18:45225 - 22413 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.00021231s
[INFO] 10.244.0.18:56234 - 50484 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000125206s
[INFO] 10.244.0.18:56234 - 29749 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000185909s
[INFO] 10.244.0.18:44864 - 56688 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000241912s
[INFO] 10.244.0.18:44864 - 21621 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000319016s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_05_02T10_18_07_0700
                    minikube.k8s.io/version=v1.33.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 02 May 2024 08:18:03 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 05 May 2024 00:50:06 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 05 May 2024 00:48:04 +0000   Thu, 02 May 2024 08:18:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 05 May 2024 00:48:04 +0000   Thu, 02 May 2024 08:18:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 05 May 2024 00:48:04 +0000   Thu, 02 May 2024 08:18:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 05 May 2024 00:48:04 +0000   Thu, 02 May 2024 08:18:16 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16302996Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16302996Ki
  pods:               110
System Info:
  Machine ID:                 7680cc34815045d9bad7c68745241a00
  System UUID:                7680cc34815045d9bad7c68745241a00
  Boot ID:                    b8f95d83-ef66-44dc-954a-683f06497fe3
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.0.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     mysql-deployment-77fb8bc69c-w4mqc           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m51s
  default                     wordpress-deployment-6777f48f9b-fz8p9       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m32s
  ingress-nginx               ingress-nginx-controller-84df5799c-dptd4    100m (2%!)(MISSING)     0 (0%!)(MISSING)      90Mi (0%!)(MISSING)        0 (0%!)(MISSING)         2d11h
  kube-system                 coredns-7db6d8ff4d-gx5kr                    100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     2d16h
  kube-system                 etcd-minikube                               100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         2d16h
  kube-system                 kube-apiserver-minikube                     250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d16h
  kube-system                 kube-controller-manager-minikube            200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d16h
  kube-system                 kube-proxy-rrg4r                            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d16h
  kube-system                 kube-scheduler-minikube                     100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d16h
  kube-system                 storage-provisioner                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d16h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%!)(MISSING)  0 (0%!)(MISSING)
  memory             260Mi (1%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[  +0.051709] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Paris not found. Is the tzdata package installed?
[  +0.847547] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.019206] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.017169] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.028107] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.060394] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.050133] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.072857] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.016572] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000004]  failed 2
[  +0.031971] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.021627] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.034433] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Paris not found. Is the tzdata package installed?
[  +1.339693] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.025518] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.070862] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.116416] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.616224] netlink: 'init': attribute type 4 has an invalid length.
[May 2 08:13] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000006] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.348614] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000007] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +1.820356] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000006] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +9.399599] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000032] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +6.514409] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000006] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +6.812769] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000006] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +6.358736] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000009] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +6.340948] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000006] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +6.353497] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000051] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +6.362751] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000008] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +6.281753] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000007] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[May 2 08:14] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000006] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +6.417495] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000006] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +6.291366] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000007] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +6.310179] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000004] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[May 3 10:07] CPU: 0 PID: 1178510 Comm: weston Not tainted 5.15.146.1-microsoft-standard-WSL2 #1
[  +0.000004] RIP: 0033:0x7f7ce5d7fcbe
[  +0.000004] Code: 48 8b 47 08 48 8d 77 10 48 8b 78 08 e9 7b bf ff ff 66 66 2e 0f 1f 84 00 00 00 00 00 f3 0f 1e fa 41 54 55 48 89 fd 48 83 ec 08 <8b> 57 28 4c 8b 67 08 85 d2 79 37 48 8d 05 50 b6 00 00 48 39 07 0f
[  +0.000002] RSP: 002b:00007ffe3bd75b10 EFLAGS: 00010202
[  +0.000003] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 00007f7ce5ee40cb
[  +0.000002] RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000
[  +0.000002] RBP: 0000000000000000 R08: 0000000000000000 R09: 00007f7ce23246fc
[  +0.000001] R10: 00007f7cbc000bd0 R11: 0000000000000293 R12: 0000000000000000
[  +0.000002] R13: 000055b9b02aab30 R14: 000055b9b06110b0 R15: 0000000000000050
[  +0.000002] FS:  00007f7ce2834ec0 GS:  0000000000000000


==> etcd [ed9b3255c75c] <==
{"level":"info","ts":"2024-05-05T00:16:57.411033Z","caller":"traceutil/trace.go:171","msg":"trace[687165687] range","detail":"{range_begin:/registry/validatingadmissionpolicybindings/; range_end:/registry/validatingadmissionpolicybindings0; response_count:0; response_revision:221502; }","duration":"276.17983ms","start":"2024-05-05T00:16:57.134822Z","end":"2024-05-05T00:16:57.411002Z","steps":["trace[687165687] 'count revisions from in-memory index tree'  (duration: 275.194273ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T00:16:59.255185Z","caller":"traceutil/trace.go:171","msg":"trace[853394240] transaction","detail":"{read_only:false; response_revision:221504; number_of_response:1; }","duration":"226.905566ms","start":"2024-05-05T00:16:59.028259Z","end":"2024-05-05T00:16:59.255165Z","steps":["trace[853394240] 'process raft request'  (duration: 226.80326ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-05T00:17:00.154219Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"528.199583ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csinodes/\" range_end:\"/registry/csinodes0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-05-05T00:17:00.154274Z","caller":"traceutil/trace.go:171","msg":"trace[1719903521] range","detail":"{range_begin:/registry/csinodes/; range_end:/registry/csinodes0; response_count:0; response_revision:221504; }","duration":"528.301489ms","start":"2024-05-05T00:16:59.62596Z","end":"2024-05-05T00:17:00.154261Z","steps":["trace[1719903521] 'count revisions from in-memory index tree'  (duration: 528.113278ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-05T00:17:00.154541Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-05T00:16:59.62594Z","time spent":"528.4827ms","remote":"127.0.0.1:42982","response type":"/etcdserverpb.KV/Range","request count":0,"request size":44,"response count":1,"response size":32,"request content":"key:\"/registry/csinodes/\" range_end:\"/registry/csinodes0\" count_only:true "}
{"level":"warn","ts":"2024-05-05T00:17:00.579489Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"797.510372ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028899818415925 > lease_revoke:<id:70cc8f3861c926eb>","response":"size:30"}
{"level":"info","ts":"2024-05-05T00:17:00.579591Z","caller":"traceutil/trace.go:171","msg":"trace[967304927] linearizableReadLoop","detail":"{readStateIndex:269384; appliedIndex:269383; }","duration":"876.448083ms","start":"2024-05-05T00:16:59.703129Z","end":"2024-05-05T00:17:00.579577Z","steps":["trace[967304927] 'read index received'  (duration: 78.013758ms)","trace[967304927] 'applied index is now lower than readState.Index'  (duration: 798.432425ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-05T00:17:00.579687Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"876.550489ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-05-05T00:17:00.579727Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"544.139894ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/podtemplates/\" range_end:\"/registry/podtemplates0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-05T00:17:00.579755Z","caller":"traceutil/trace.go:171","msg":"trace[1438597181] range","detail":"{range_begin:/registry/podtemplates/; range_end:/registry/podtemplates0; response_count:0; response_revision:221504; }","duration":"544.200397ms","start":"2024-05-05T00:17:00.035547Z","end":"2024-05-05T00:17:00.579747Z","steps":["trace[1438597181] 'agreement among raft nodes before linearized reading'  (duration: 544.112392ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-05T00:17:00.579823Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-05T00:17:00.035532Z","time spent":"544.275402ms","remote":"127.0.0.1:42728","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":0,"response size":30,"request content":"key:\"/registry/podtemplates/\" range_end:\"/registry/podtemplates0\" count_only:true "}
{"level":"info","ts":"2024-05-05T00:17:00.579753Z","caller":"traceutil/trace.go:171","msg":"trace[488475943] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:221504; }","duration":"876.652995ms","start":"2024-05-05T00:16:59.703086Z","end":"2024-05-05T00:17:00.579739Z","steps":["trace[488475943] 'agreement among raft nodes before linearized reading'  (duration: 876.557989ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-05T00:17:00.580512Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-05T00:16:59.703067Z","time spent":"877.43004ms","remote":"127.0.0.1:42574","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-05-05T00:17:01.138852Z","caller":"traceutil/trace.go:171","msg":"trace[1676720283] transaction","detail":"{read_only:false; response_revision:221505; number_of_response:1; }","duration":"230.280459ms","start":"2024-05-05T00:17:00.908552Z","end":"2024-05-05T00:17:01.138833Z","steps":["trace[1676720283] 'process raft request'  (duration: 230.111849ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-05T00:17:01.589431Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"117.225198ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2024-05-05T00:17:01.589514Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"170.239228ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/\" range_end:\"/registry/deployments0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2024-05-05T00:17:01.589612Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"330.91881ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:604"}
{"level":"warn","ts":"2024-05-05T00:17:01.589626Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"134.467384ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/resourcequotas/\" range_end:\"/registry/resourcequotas0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-05T00:17:01.589653Z","caller":"traceutil/trace.go:171","msg":"trace[328821467] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:221505; }","duration":"330.985914ms","start":"2024-05-05T00:17:01.258651Z","end":"2024-05-05T00:17:01.589637Z","steps":["trace[328821467] 'range keys from in-memory index tree'  (duration: 330.848605ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T00:17:01.589551Z","caller":"traceutil/trace.go:171","msg":"trace[639314183] range","detail":"{range_begin:/registry/deployments/; range_end:/registry/deployments0; response_count:0; response_revision:221505; }","duration":"170.307932ms","start":"2024-05-05T00:17:01.419233Z","end":"2024-05-05T00:17:01.589541Z","steps":["trace[639314183] 'count revisions from in-memory index tree'  (duration: 170.142423ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-05T00:17:01.58969Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-05T00:17:01.258638Z","time spent":"331.040917ms","remote":"127.0.0.1:42768","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":628,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2024-05-05T00:17:01.589653Z","caller":"traceutil/trace.go:171","msg":"trace[373250548] range","detail":"{range_begin:/registry/resourcequotas/; range_end:/registry/resourcequotas0; response_count:0; response_revision:221505; }","duration":"134.524287ms","start":"2024-05-05T00:17:01.455122Z","end":"2024-05-05T00:17:01.589646Z","steps":["trace[373250548] 'count revisions from in-memory index tree'  (duration: 134.386879ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T00:17:01.589487Z","caller":"traceutil/trace.go:171","msg":"trace[364329577] range","detail":"{range_begin:/registry/configmaps/; range_end:/registry/configmaps0; response_count:0; response_revision:221505; }","duration":"117.294702ms","start":"2024-05-05T00:17:01.47218Z","end":"2024-05-05T00:17:01.589474Z","steps":["trace[364329577] 'count revisions from in-memory index tree'  (duration: 117.140694ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T00:17:01.725385Z","caller":"traceutil/trace.go:171","msg":"trace[1592854075] transaction","detail":"{read_only:false; response_revision:221506; number_of_response:1; }","duration":"131.609021ms","start":"2024-05-05T00:17:01.593757Z","end":"2024-05-05T00:17:01.725366Z","steps":["trace[1592854075] 'process raft request'  (duration: 131.364607ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T00:21:04.910492Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":221402}
{"level":"info","ts":"2024-05-05T00:21:04.915059Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":221402,"took":"4.154634ms","hash":2351994884,"current-db-size-bytes":4964352,"current-db-size":"5.0 MB","current-db-size-in-use-bytes":2195456,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2024-05-05T00:21:04.915116Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2351994884,"revision":221402,"compact-revision":221069}
{"level":"info","ts":"2024-05-05T00:26:04.91035Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":221741}
{"level":"info","ts":"2024-05-05T00:26:04.917727Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":221741,"took":"6.48583ms","hash":795372092,"current-db-size-bytes":4964352,"current-db-size":"5.0 MB","current-db-size-in-use-bytes":1822720,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-05-05T00:26:04.917808Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":795372092,"revision":221741,"compact-revision":221402}
{"level":"info","ts":"2024-05-05T00:26:17.176689Z","caller":"etcdserver/server.go:1401","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":270027,"local-member-snapshot-index":260026,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-05-05T00:26:17.19001Z","caller":"etcdserver/server.go:2420","msg":"saved snapshot","snapshot-index":270027}
{"level":"info","ts":"2024-05-05T00:26:17.190144Z","caller":"etcdserver/server.go:2450","msg":"compacted Raft logs","compact-index":265027}
{"level":"info","ts":"2024-05-05T00:26:28.351985Z","caller":"fileutil/purge.go:96","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000002-0000000000035b76.snap"}
{"level":"info","ts":"2024-05-05T00:31:04.904326Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":222021}
{"level":"info","ts":"2024-05-05T00:31:04.908378Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":222021,"took":"3.661194ms","hash":780435934,"current-db-size-bytes":4964352,"current-db-size":"5.0 MB","current-db-size-in-use-bytes":1466368,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-05T00:31:04.908433Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":780435934,"revision":222021,"compact-revision":221741}
{"level":"info","ts":"2024-05-05T00:33:55.955377Z","caller":"traceutil/trace.go:171","msg":"trace[1729038549] transaction","detail":"{read_only:false; response_revision:222499; number_of_response:1; }","duration":"104.814612ms","start":"2024-05-05T00:33:55.850539Z","end":"2024-05-05T00:33:55.955353Z","steps":["trace[1729038549] 'process raft request'  (duration: 57.685834ms)","trace[1729038549] 'compare'  (duration: 46.98117ms)"],"step_count":2}
{"level":"info","ts":"2024-05-05T00:36:04.900973Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":222306}
{"level":"info","ts":"2024-05-05T00:36:04.905119Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":222306,"took":"3.738389ms","hash":313976261,"current-db-size-bytes":4964352,"current-db-size":"5.0 MB","current-db-size-in-use-bytes":1736704,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-05-05T00:36:04.905194Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":313976261,"revision":222306,"compact-revision":222021}
{"level":"info","ts":"2024-05-05T00:36:37.043809Z","caller":"traceutil/trace.go:171","msg":"trace[1527526683] transaction","detail":"{read_only:false; response_revision:222649; number_of_response:1; }","duration":"103.026241ms","start":"2024-05-05T00:36:36.940762Z","end":"2024-05-05T00:36:37.043789Z","steps":["trace[1527526683] 'process raft request'  (duration: 102.883533ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T00:41:04.899255Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":222620}
{"level":"info","ts":"2024-05-05T00:41:04.908081Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":222620,"took":"8.39243ms","hash":650314527,"current-db-size-bytes":4964352,"current-db-size":"5.0 MB","current-db-size-in-use-bytes":1781760,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-05-05T00:41:04.90817Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":650314527,"revision":222620,"compact-revision":222306}
{"level":"info","ts":"2024-05-05T00:46:04.897124Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":222902}
{"level":"info","ts":"2024-05-05T00:46:04.909491Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":222902,"took":"11.979511ms","hash":3437871626,"current-db-size-bytes":4964352,"current-db-size":"5.0 MB","current-db-size-in-use-bytes":1789952,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-05-05T00:46:04.909575Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3437871626,"revision":222902,"compact-revision":222620}
{"level":"info","ts":"2024-05-05T00:47:46.136204Z","caller":"traceutil/trace.go:171","msg":"trace[1769893830] transaction","detail":"{read_only:false; response_revision:223449; number_of_response:1; }","duration":"106.479869ms","start":"2024-05-05T00:47:46.029707Z","end":"2024-05-05T00:47:46.136187Z","steps":["trace[1769893830] 'process raft request'  (duration: 106.442967ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T00:47:46.136263Z","caller":"traceutil/trace.go:171","msg":"trace[1391188878] transaction","detail":"{read_only:false; response_revision:223448; number_of_response:1; }","duration":"114.537775ms","start":"2024-05-05T00:47:46.02171Z","end":"2024-05-05T00:47:46.136248Z","steps":["trace[1391188878] 'process raft request'  (duration: 114.398068ms)"],"step_count":1}
{"level":"info","ts":"2024-05-05T00:47:46.136715Z","caller":"traceutil/trace.go:171","msg":"trace[1107521551] transaction","detail":"{read_only:false; response_revision:223447; number_of_response:1; }","duration":"122.547379ms","start":"2024-05-05T00:47:46.01415Z","end":"2024-05-05T00:47:46.136698Z","steps":["trace[1107521551] 'process raft request'  (duration: 58.577553ms)","trace[1107521551] 'compare'  (duration: 63.241789ms)"],"step_count":2}
{"level":"info","ts":"2024-05-05T00:49:24.528417Z","caller":"traceutil/trace.go:171","msg":"trace[1398539115] linearizableReadLoop","detail":"{readStateIndex:271840; appliedIndex:271839; }","duration":"189.933315ms","start":"2024-05-05T00:49:24.338465Z","end":"2024-05-05T00:49:24.528398Z","steps":["trace[1398539115] 'read index received'  (duration: 189.838709ms)","trace[1398539115] 'applied index is now lower than readState.Index'  (duration: 93.506µs)"],"step_count":2}
{"level":"warn","ts":"2024-05-05T00:49:24.52873Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"190.074324ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-fb435956-acaf-4cc3-8272-28f2292dbb3d\" ","response":"range_response_count:1 size:1227"}
{"level":"info","ts":"2024-05-05T00:49:24.528805Z","caller":"traceutil/trace.go:171","msg":"trace[429036763] range","detail":"{range_begin:/registry/persistentvolumes/pvc-fb435956-acaf-4cc3-8272-28f2292dbb3d; range_end:; response_count:1; response_revision:223553; }","duration":"190.252835ms","start":"2024-05-05T00:49:24.338539Z","end":"2024-05-05T00:49:24.528792Z","steps":["trace[429036763] 'agreement among raft nodes before linearized reading'  (duration: 189.994819ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-05T00:49:24.528869Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"190.088325ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-06689616-9984-48cb-8682-352cb0daec1b\" ","response":"range_response_count:1 size:1214"}
{"level":"info","ts":"2024-05-05T00:49:24.528911Z","caller":"traceutil/trace.go:171","msg":"trace[2049273375] range","detail":"{range_begin:/registry/persistentvolumes/pvc-06689616-9984-48cb-8682-352cb0daec1b; range_end:; response_count:1; response_revision:223553; }","duration":"190.148329ms","start":"2024-05-05T00:49:24.338752Z","end":"2024-05-05T00:49:24.5289Z","steps":["trace[2049273375] 'agreement among raft nodes before linearized reading'  (duration: 190.048722ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-05T00:49:24.530978Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"190.118427ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-e6191fdd-7ab7-47b8-a192-4142cf37c720\" ","response":"range_response_count:1 size:1221"}
{"level":"info","ts":"2024-05-05T00:49:24.531082Z","caller":"traceutil/trace.go:171","msg":"trace[1680764190] range","detail":"{range_begin:/registry/persistentvolumes/pvc-e6191fdd-7ab7-47b8-a192-4142cf37c720; range_end:; response_count:1; response_revision:223553; }","duration":"192.622583ms","start":"2024-05-05T00:49:24.338437Z","end":"2024-05-05T00:49:24.531059Z","steps":["trace[1680764190] 'agreement among raft nodes before linearized reading'  (duration: 190.043622ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-05T00:49:24.532788Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"160.018555ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:604"}
{"level":"info","ts":"2024-05-05T00:49:24.532882Z","caller":"traceutil/trace.go:171","msg":"trace[1844661471] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:223553; }","duration":"164.308521ms","start":"2024-05-05T00:49:24.368555Z","end":"2024-05-05T00:49:24.532864Z","steps":["trace[1844661471] 'agreement among raft nodes before linearized reading'  (duration: 160.019854ms)"],"step_count":1}


==> kernel <==
 00:50:16 up 5 days, 11:38,  0 users,  load average: 0.36, 0.44, 0.43
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [93f676895d83] <==
Trace[2139015124]: [533.100385ms] [533.100385ms] END
I0504 23:28:10.910460       1 trace.go:236] Trace[1498955891]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:756d0e24-4873-4d52-937c-6af8909fa01d,client:192.168.49.2,api-group:coordination.k8s.io,api-version:v1,name:minikube,subresource:,namespace:kube-node-lease,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (04-May-2024 23:28:10.114) (total time: 795ms):
Trace[1498955891]: ["GuaranteedUpdate etcd3" audit-id:756d0e24-4873-4d52-937c-6af8909fa01d,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 795ms (23:28:10.115)
Trace[1498955891]:  ---"Txn call completed" 794ms (23:28:10.910)]
Trace[1498955891]: [795.544328ms] [795.544328ms] END
I0504 23:28:14.025160       1 trace.go:236] Trace[641149518]: "Update" accept:application/json, */*,audit-id:865c86a8-6f65-42b5-806d-75b1936c1ef2,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (04-May-2024 23:28:13.371) (total time: 653ms):
Trace[641149518]: ["GuaranteedUpdate etcd3" audit-id:865c86a8-6f65-42b5-806d-75b1936c1ef2,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 653ms (23:28:13.371)
Trace[641149518]:  ---"Txn call completed" 652ms (23:28:14.024)]
Trace[641149518]: [653.899451ms] [653.899451ms] END
I0504 23:28:15.330055       1 trace.go:236] Trace[610939165]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (04-May-2024 23:28:14.202) (total time: 1127ms):
Trace[610939165]: ---"initial value restored" 452ms (23:28:14.655)
Trace[610939165]: ---"Transaction prepared" 310ms (23:28:14.965)
Trace[610939165]: ---"Txn call completed" 364ms (23:28:15.329)
Trace[610939165]: [1.127205181s] [1.127205181s] END
I0504 23:28:18.028545       1 trace.go:236] Trace[1419551134]: "Get" accept:application/json, */*,audit-id:9445b214-b2b2-4943-9b2c-e03884caab50,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (04-May-2024 23:28:16.028) (total time: 2000ms):
Trace[1419551134]: ---"About to write a response" 1999ms (23:28:18.028)
Trace[1419551134]: [2.000061635s] [2.000061635s] END
I0504 23:28:18.028743       1 trace.go:236] Trace[833089365]: "Get" accept:application/json, */*,audit-id:5c4d1a51-4e76-4e93-a17a-8efdfa2d29a9,client:10.244.0.24,api-group:coordination.k8s.io,api-version:v1,name:ingress-nginx-leader,subresource:,namespace:ingress-nginx,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader,user-agent:nginx-ingress-controller/v1.10.0 (linux/amd64) ingress-nginx/71f78d49f0a496c31d4c19f095469f3f23900f8a,verb:GET (04-May-2024 23:28:16.640) (total time: 1388ms):
Trace[833089365]: ---"About to write a response" 1388ms (23:28:18.028)
Trace[833089365]: [1.388095948s] [1.388095948s] END
I0504 23:28:24.570112       1 trace.go:236] Trace[1722714568]: "Update" accept:application/json, */*,audit-id:3584b0ef-139c-448e-8dc0-c0987e1b33a3,client:10.244.0.24,api-group:coordination.k8s.io,api-version:v1,name:ingress-nginx-leader,subresource:,namespace:ingress-nginx,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader,user-agent:nginx-ingress-controller/v1.10.0 (linux/amd64) ingress-nginx/71f78d49f0a496c31d4c19f095469f3f23900f8a,verb:PUT (04-May-2024 23:28:18.030) (total time: 6539ms):
Trace[1722714568]: ["GuaranteedUpdate etcd3" audit-id:3584b0ef-139c-448e-8dc0-c0987e1b33a3,key:/leases/ingress-nginx/ingress-nginx-leader,type:*coordination.Lease,resource:leases.coordination.k8s.io 6539ms (23:28:18.031)
Trace[1722714568]:  ---"Txn call completed" 6538ms (23:28:24.569)]
Trace[1722714568]: [6.539249569s] [6.539249569s] END
I0504 23:28:24.570593       1 trace.go:236] Trace[1376854154]: "Update" accept:application/json, */*,audit-id:42b9210b-8c1c-4f67-b6fd-674eee050081,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (04-May-2024 23:28:18.030) (total time: 6539ms):
Trace[1376854154]: ["GuaranteedUpdate etcd3" audit-id:42b9210b-8c1c-4f67-b6fd-674eee050081,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 6539ms (23:28:18.030)
Trace[1376854154]:  ---"Txn call completed" 6538ms (23:28:24.570)]
Trace[1376854154]: [6.5398377s] [6.5398377s] END
I0504 23:28:24.593759       1 trace.go:236] Trace[664278284]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fc662211-863b-47a0-8518-b857cf36d768,client:::1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (04-May-2024 23:28:18.716) (total time: 5877ms):
Trace[664278284]: ["GuaranteedUpdate etcd3" audit-id:fc662211-863b-47a0-8518-b857cf36d768,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 5876ms (23:28:18.716)
Trace[664278284]:  ---"Txn call completed" 5874ms (23:28:24.593)]
Trace[664278284]: [5.877184521s] [5.877184521s] END
I0504 23:28:24.692704       1 trace.go:236] Trace[1239697311]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:3b8f4be2-8fb2-4328-9001-bce80aa6e25c,client:192.168.49.2,api-group:,api-version:v1,name:kube-apiserver-minikube.17cba048d82d30fd,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/kube-apiserver-minikube.17cba048d82d30fd,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PATCH (04-May-2024 23:28:18.033) (total time: 6658ms):
Trace[1239697311]: ["GuaranteedUpdate etcd3" audit-id:3b8f4be2-8fb2-4328-9001-bce80aa6e25c,key:/events/kube-system/kube-apiserver-minikube.17cba048d82d30fd,type:*core.Event,resource:events 6658ms (23:28:18.033)
Trace[1239697311]:  ---"initial value restored" 6658ms (23:28:24.692)]
Trace[1239697311]: [6.658912677s] [6.658912677s] END
I0504 23:28:24.693286       1 trace.go:236] Trace[1146192635]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:2948b5db-f91f-4aa8-b268-20992096497d,client:192.168.49.2,api-group:coordination.k8s.io,api-version:v1,name:minikube,subresource:,namespace:kube-node-lease,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (04-May-2024 23:28:20.921) (total time: 3771ms):
Trace[1146192635]: ["GuaranteedUpdate etcd3" audit-id:2948b5db-f91f-4aa8-b268-20992096497d,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 3771ms (23:28:20.921)
Trace[1146192635]:  ---"Txn call completed" 3770ms (23:28:24.693)]
Trace[1146192635]: [3.771999467s] [3.771999467s] END
I0504 23:28:24.694578       1 trace.go:236] Trace[1579289524]: "List" accept:application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json,audit-id:7c6b2c9b-1abf-4fcf-aae6-b0ab0e26bacb,client:192.168.49.1,api-group:,api-version:v1,name:,subresource:,namespace:default,protocol:HTTP/2.0,resource:pods,scope:namespace,url:/api/v1/namespaces/default/pods,user-agent:kubectl.exe/v1.29.2 (windows/amd64) kubernetes/4b8e819,verb:LIST (04-May-2024 23:28:20.225) (total time: 4468ms):
Trace[1579289524]: ["List(recursive=true) etcd3" audit-id:7c6b2c9b-1abf-4fcf-aae6-b0ab0e26bacb,key:/pods/default,resourceVersion:,resourceVersionMatch:,limit:500,continue: 4468ms (23:28:20.226)]
Trace[1579289524]: [4.468567722s] [4.468567722s] END
I0504 23:28:24.712564       1 trace.go:236] Trace[1273707264]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bb128e4c-12ac-442d-ba14-074dcda5e213,client:192.168.49.2,api-group:,api-version:v1,name:mysql-pvc,subresource:,namespace:default,protocol:HTTP/2.0,resource:persistentvolumeclaims,scope:resource,url:/api/v1/namespaces/default/persistentvolumeclaims/mysql-pvc,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:persistent-volume-binder,verb:GET (04-May-2024 23:28:24.210) (total time: 501ms):
Trace[1273707264]: ---"About to write a response" 501ms (23:28:24.712)
Trace[1273707264]: [501.555126ms] [501.555126ms] END
I0504 23:28:24.760405       1 trace.go:236] Trace[437290669]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (04-May-2024 23:28:24.204) (total time: 556ms):
Trace[437290669]: ---"initial value restored" 507ms (23:28:24.711)
Trace[437290669]: [556.285509ms] [556.285509ms] END
I0504 23:29:44.377965       1 alloc.go:330] "allocated clusterIPs" service="default/mysql" clusterIPs={"IPv4":"10.99.214.244"}
I0504 23:30:54.702806       1 alloc.go:330] "allocated clusterIPs" service="default/wordpress" clusterIPs={"IPv4":"10.96.251.229"}
I0504 23:41:03.999199       1 alloc.go:330] "allocated clusterIPs" service="default/wordpress" clusterIPs={"IPv4":"10.105.14.104"}
I0504 23:48:12.453660       1 alloc.go:330] "allocated clusterIPs" service="default/mysql" clusterIPs={"IPv4":"10.101.126.225"}
I0504 23:48:13.810395       1 alloc.go:330] "allocated clusterIPs" service="default/wordpress" clusterIPs={"IPv4":"10.108.40.153"}
I0505 00:16:04.805391       1 alloc.go:330] "allocated clusterIPs" service="default/mysql" clusterIPs={"IPv4":"10.99.2.246"}
I0505 00:16:06.315493       1 alloc.go:330] "allocated clusterIPs" service="default/wordpress" clusterIPs={"IPv4":"10.96.147.1"}
I0505 00:43:21.248272       1 alloc.go:330] "allocated clusterIPs" service="default/mysql" clusterIPs={"IPv4":"10.108.215.128"}
I0505 00:43:22.732655       1 alloc.go:330] "allocated clusterIPs" service="default/wordpress" clusterIPs={"IPv4":"10.97.197.24"}
I0505 00:46:25.742282       1 alloc.go:330] "allocated clusterIPs" service="default/mysql" clusterIPs={"IPv4":"10.104.177.52"}
I0505 00:47:45.455603       1 alloc.go:330] "allocated clusterIPs" service="default/wordpress" clusterIPs={"IPv4":"10.104.21.231"}


==> kube-controller-manager [4eae87c45fb2] <==
I0505 00:10:01.741996       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-546997c855" duration="39.902µs"
I0505 00:10:01.758010       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-546997c855" duration="40.403µs"
I0505 00:10:01.764581       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-546997c855" duration="42.702µs"
I0505 00:14:18.586218       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-7d96567c" duration="16.502µs"
I0505 00:14:18.597005       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-669654b585" duration="6.401µs"
I0505 00:14:18.597470       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5695dfb9b8" duration="6.501µs"
I0505 00:14:18.597534       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-546997c855" duration="2.9µs"
I0505 00:14:18.645907       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="4.9µs"
I0505 00:16:04.445991       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="79.261579ms"
I0505 00:16:04.471661       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="25.283999ms"
I0505 00:16:04.471767       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="47.601µs"
I0505 00:16:05.944270       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="84.057292ms"
I0505 00:16:05.975271       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="30.898932ms"
I0505 00:16:06.013996       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="38.635616ms"
I0505 00:16:06.018653       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="4.550808ms"
I0505 00:16:06.057094       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="43.201µs"
I0505 00:16:06.091832       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="43.001µs"
I0505 00:16:07.074578       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="43.701µs"
I0505 00:16:07.118263       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="43.101µs"
I0505 00:16:08.874569       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="40.345856ms"
I0505 00:16:08.874674       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="54.902µs"
I0505 00:17:03.288416       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="9.703554ms"
I0505 00:17:03.289630       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="37.603µs"
I0505 00:33:52.761066       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="14.201µs"
I0505 00:33:52.768972       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="30.401µs"
I0505 00:43:20.949017       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="87.922868ms"
I0505 00:43:20.966578       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="17.479008ms"
I0505 00:43:20.966703       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="41.602µs"
I0505 00:43:22.273881       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="92.791921ms"
I0505 00:43:22.295628       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="21.453915ms"
I0505 00:43:22.295872       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="42.102µs"
I0505 00:43:22.333305       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="49.203µs"
I0505 00:43:22.371344       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="40.602µs"
I0505 00:43:24.573580       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="39.392744ms"
I0505 00:43:24.573963       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="34.902µs"
I0505 00:43:31.205594       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="48.503µs"
I0505 00:43:33.369264       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="118.706µs"
I0505 00:43:33.445578       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="41.802µs"
I0505 00:43:35.834527       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="18.583089ms"
I0505 00:43:35.834653       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="66.604µs"
I0505 00:46:23.510467       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-5cf59d7c46" duration="5.7µs"
I0505 00:46:23.610669       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="5.8µs"
I0505 00:46:25.248789       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="81.134921ms"
I0505 00:46:25.310788       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="61.91012ms"
I0505 00:46:25.334792       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="23.895543ms"
I0505 00:46:25.335110       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="77.804µs"
I0505 00:46:28.447902       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="41.502µs"
I0505 00:46:32.447781       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="80.904µs"
I0505 00:46:32.471712       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="46.402µs"
I0505 00:46:34.217542       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="23.718138ms"
I0505 00:46:34.218493       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-77fb8bc69c" duration="42.402µs"
I0505 00:47:44.751749       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-6777f48f9b" duration="121.313516ms"
I0505 00:47:44.811683       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-6777f48f9b" duration="59.856218ms"
I0505 00:47:44.811872       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-6777f48f9b" duration="83.304µs"
I0505 00:47:44.812054       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-6777f48f9b" duration="30.802µs"
I0505 00:47:46.006965       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-6777f48f9b" duration="42.402µs"
I0505 00:47:48.519905       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-6777f48f9b" duration="39.602µs"
I0505 00:47:48.537005       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-6777f48f9b" duration="42.302µs"
I0505 00:47:50.276107       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-6777f48f9b" duration="13.288378ms"
I0505 00:47:50.276346       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-6777f48f9b" duration="80.804µs"


==> kube-proxy [da40952e2602] <==
I0502 08:18:22.150688       1 server_linux.go:69] "Using iptables proxy"
I0502 08:18:22.163326       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0502 08:18:22.198565       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0502 08:18:22.198649       1 server_linux.go:165] "Using iptables Proxier"
I0502 08:18:22.201573       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0502 08:18:22.201618       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0502 08:18:22.201646       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0502 08:18:22.201965       1 server.go:872] "Version info" version="v1.30.0"
I0502 08:18:22.201996       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0502 08:18:22.203449       1 config.go:101] "Starting endpoint slice config controller"
I0502 08:18:22.203681       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0502 08:18:22.203747       1 config.go:192] "Starting service config controller"
I0502 08:18:22.203755       1 shared_informer.go:313] Waiting for caches to sync for service config
I0502 08:18:22.204699       1 config.go:319] "Starting node config controller"
I0502 08:18:22.204750       1 shared_informer.go:313] Waiting for caches to sync for node config
I0502 08:18:22.304417       1 shared_informer.go:320] Caches are synced for service config
I0502 08:18:22.304479       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0502 08:18:22.304865       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [fa96e01ce6fd] <==
E0502 08:18:03.686608       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0502 08:18:03.688205       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0502 08:18:03.688744       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0502 08:18:03.688608       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0502 08:18:03.689429       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0502 08:18:03.688691       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0502 08:18:03.694864       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0502 08:18:04.532749       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0502 08:18:04.532810       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0502 08:18:04.622530       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0502 08:18:04.622867       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0502 08:18:04.635651       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0502 08:18:04.635712       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0502 08:18:04.735754       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0502 08:18:04.736200       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0502 08:18:04.768858       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0502 08:18:04.769112       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0502 08:18:04.867227       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0502 08:18:04.867272       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0502 08:18:04.931714       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0502 08:18:04.931755       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0502 08:18:05.028029       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0502 08:18:05.028103       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0502 08:18:05.050697       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0502 08:18:05.050859       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0502 08:18:05.054575       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0502 08:18:05.054637       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0502 08:18:05.110196       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0502 08:18:05.110270       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0502 08:18:05.121240       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0502 08:18:05.121276       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
I0502 08:18:07.472702       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0503 13:01:48.159813       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-6bf85cb97b-jlgnq"
E0503 13:01:48.160383       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-6bf85cb97b-jlgnq"
E0503 13:13:02.484388       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mysql-pvc\": could not find v1.PersistentVolumeClaim \"default/mysql-pvc\"" pod="default/mysql-deployment-5bf8c6cdcd-62zr5"
E0503 13:13:02.484459       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mysql-pvc\": could not find v1.PersistentVolumeClaim \"default/mysql-pvc\"" pod="default/mysql-deployment-5bf8c6cdcd-62zr5"
E0503 13:13:45.316401       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-7b4c8bc594-5m9ch"
E0503 13:13:45.316477       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-7b4c8bc594-5m9ch"
E0503 13:24:33.349867       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mysql-pvc\": could not find v1.PersistentVolumeClaim \"default/mysql-pvc\"" pod="default/mysql-deployment-5475bd5cb6-krzwx"
E0503 13:24:33.349937       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mysql-pvc\": could not find v1.PersistentVolumeClaim \"default/mysql-pvc\"" pod="default/mysql-deployment-5475bd5cb6-krzwx"
E0503 13:25:53.277372       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-5f956d8c56-d74jr"
E0503 13:25:53.277489       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-5f956d8c56-d74jr"
E0503 14:09:22.000472       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mysql-pvc\": could not find v1.PersistentVolumeClaim \"default/mysql-pvc\"" pod="default/mysql-deployment-5475bd5cb6-4httv"
E0503 14:09:22.001443       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mysql-pvc\": could not find v1.PersistentVolumeClaim \"default/mysql-pvc\"" pod="default/mysql-deployment-5475bd5cb6-4httv"
E0503 14:11:03.352133       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-7d68df49-gcllf"
E0503 14:11:03.352200       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-7d68df49-gcllf"
E0503 14:14:19.997865       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-7644dc45f-wvhlj"
E0503 14:14:19.997910       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-7644dc45f-wvhlj"
E0503 14:37:20.718801       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-7644dc45f-25m8w"
E0503 14:37:20.718864       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-7644dc45f-25m8w"
E0504 22:01:32.117031       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mysql-pvc\": could not find v1.PersistentVolumeClaim \"default/mysql-pvc\"" pod="default/mysql-deployment-5475bd5cb6-8ddlm"
E0504 22:01:32.117506       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mysql-pvc\": could not find v1.PersistentVolumeClaim \"default/mysql-pvc\"" pod="default/mysql-deployment-5475bd5cb6-8ddlm"
E0504 22:01:34.117746       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-7644dc45f-pws6w"
E0504 22:01:34.117811       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-7644dc45f-pws6w"
E0504 23:24:51.625058       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mysql-pvc\": could not find v1.PersistentVolumeClaim \"default/mysql-pvc\"" pod="default/mysql-deployment-77fb8bc69c-dqrpm"
E0504 23:24:51.625390       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mysql-pvc\": could not find v1.PersistentVolumeClaim \"default/mysql-pvc\"" pod="default/mysql-deployment-77fb8bc69c-dqrpm"
E0504 23:29:45.998111       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mysql-pvc\": could not find v1.PersistentVolumeClaim \"default/mysql-pvc\"" pod="default/mysql-deployment-77fb8bc69c-t85mg"
E0504 23:29:45.998514       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mysql-pvc\": could not find v1.PersistentVolumeClaim \"default/mysql-pvc\"" pod="default/mysql-deployment-77fb8bc69c-t85mg"
E0504 23:30:56.792490       1 schedule_one.go:161] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-7d96567c-s889z"
E0504 23:30:56.792560       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/wordpress-pvc\": could not find v1.PersistentVolumeClaim \"default/wordpress-pvc\"" pod="default/wordpress-deployment-7d96567c-s889z"


==> kubelet <==
May 05 00:33:54 minikube kubelet[2279]: I0505 00:33:54.341807    2279 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/373b80d8-c5cf-48c9-b93b-5a6f07d0b590-pvc-2c2171a1-c9c5-46aa-a4ee-9fe7ee3a92aa" (OuterVolumeSpecName: "mysql-persistent-storage") pod "373b80d8-c5cf-48c9-b93b-5a6f07d0b590" (UID: "373b80d8-c5cf-48c9-b93b-5a6f07d0b590"). InnerVolumeSpecName "pvc-2c2171a1-c9c5-46aa-a4ee-9fe7ee3a92aa". PluginName "kubernetes.io/host-path", VolumeGidValue ""
May 05 00:33:54 minikube kubelet[2279]: I0505 00:33:54.341856    2279 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"wordpress-persistent-storage\" (UniqueName: \"kubernetes.io/host-path/f81aa96a-cf8f-43bf-b3cd-709231106cdd-pvc-216083ba-3722-4fce-aaf0-ff24d11ca486\") pod \"f81aa96a-cf8f-43bf-b3cd-709231106cdd\" (UID: \"f81aa96a-cf8f-43bf-b3cd-709231106cdd\") "
May 05 00:33:54 minikube kubelet[2279]: I0505 00:33:54.341968    2279 reconciler_common.go:289] "Volume detached for volume \"pvc-2c2171a1-c9c5-46aa-a4ee-9fe7ee3a92aa\" (UniqueName: \"kubernetes.io/host-path/373b80d8-c5cf-48c9-b93b-5a6f07d0b590-pvc-2c2171a1-c9c5-46aa-a4ee-9fe7ee3a92aa\") on node \"minikube\" DevicePath \"\""
May 05 00:33:54 minikube kubelet[2279]: I0505 00:33:54.342035    2279 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/f81aa96a-cf8f-43bf-b3cd-709231106cdd-pvc-216083ba-3722-4fce-aaf0-ff24d11ca486" (OuterVolumeSpecName: "wordpress-persistent-storage") pod "f81aa96a-cf8f-43bf-b3cd-709231106cdd" (UID: "f81aa96a-cf8f-43bf-b3cd-709231106cdd"). InnerVolumeSpecName "pvc-216083ba-3722-4fce-aaf0-ff24d11ca486". PluginName "kubernetes.io/host-path", VolumeGidValue ""
May 05 00:33:54 minikube kubelet[2279]: I0505 00:33:54.347740    2279 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/f81aa96a-cf8f-43bf-b3cd-709231106cdd-kube-api-access-5dqxx" (OuterVolumeSpecName: "kube-api-access-5dqxx") pod "f81aa96a-cf8f-43bf-b3cd-709231106cdd" (UID: "f81aa96a-cf8f-43bf-b3cd-709231106cdd"). InnerVolumeSpecName "kube-api-access-5dqxx". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 05 00:33:54 minikube kubelet[2279]: I0505 00:33:54.347753    2279 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/373b80d8-c5cf-48c9-b93b-5a6f07d0b590-kube-api-access-vv6hc" (OuterVolumeSpecName: "kube-api-access-vv6hc") pod "373b80d8-c5cf-48c9-b93b-5a6f07d0b590" (UID: "373b80d8-c5cf-48c9-b93b-5a6f07d0b590"). InnerVolumeSpecName "kube-api-access-vv6hc". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 05 00:33:54 minikube kubelet[2279]: I0505 00:33:54.442283    2279 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-vv6hc\" (UniqueName: \"kubernetes.io/projected/373b80d8-c5cf-48c9-b93b-5a6f07d0b590-kube-api-access-vv6hc\") on node \"minikube\" DevicePath \"\""
May 05 00:33:54 minikube kubelet[2279]: I0505 00:33:54.442347    2279 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-5dqxx\" (UniqueName: \"kubernetes.io/projected/f81aa96a-cf8f-43bf-b3cd-709231106cdd-kube-api-access-5dqxx\") on node \"minikube\" DevicePath \"\""
May 05 00:33:54 minikube kubelet[2279]: I0505 00:33:54.442366    2279 reconciler_common.go:289] "Volume detached for volume \"pvc-216083ba-3722-4fce-aaf0-ff24d11ca486\" (UniqueName: \"kubernetes.io/host-path/f81aa96a-cf8f-43bf-b3cd-709231106cdd-pvc-216083ba-3722-4fce-aaf0-ff24d11ca486\") on node \"minikube\" DevicePath \"\""
May 05 00:33:55 minikube kubelet[2279]: I0505 00:33:55.185202    2279 scope.go:117] "RemoveContainer" containerID="ea80fd102613c316085b04205b8170076bdfccd2994637d30e5f26790eff45e5"
May 05 00:33:55 minikube kubelet[2279]: I0505 00:33:55.223897    2279 scope.go:117] "RemoveContainer" containerID="ea80fd102613c316085b04205b8170076bdfccd2994637d30e5f26790eff45e5"
May 05 00:33:55 minikube kubelet[2279]: E0505 00:33:55.228717    2279 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: ea80fd102613c316085b04205b8170076bdfccd2994637d30e5f26790eff45e5" containerID="ea80fd102613c316085b04205b8170076bdfccd2994637d30e5f26790eff45e5"
May 05 00:33:55 minikube kubelet[2279]: I0505 00:33:55.228895    2279 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"ea80fd102613c316085b04205b8170076bdfccd2994637d30e5f26790eff45e5"} err="failed to get container status \"ea80fd102613c316085b04205b8170076bdfccd2994637d30e5f26790eff45e5\": rpc error: code = Unknown desc = Error response from daemon: No such container: ea80fd102613c316085b04205b8170076bdfccd2994637d30e5f26790eff45e5"
May 05 00:33:55 minikube kubelet[2279]: I0505 00:33:55.228928    2279 scope.go:117] "RemoveContainer" containerID="31fc1302f4560729b9bacd999f17c7d0deeb1e8f2648ac1bf5dcb59a236b9641"
May 05 00:33:55 minikube kubelet[2279]: I0505 00:33:55.480600    2279 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="373b80d8-c5cf-48c9-b93b-5a6f07d0b590" path="/var/lib/kubelet/pods/373b80d8-c5cf-48c9-b93b-5a6f07d0b590/volumes"
May 05 00:33:55 minikube kubelet[2279]: I0505 00:33:55.481180    2279 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="f81aa96a-cf8f-43bf-b3cd-709231106cdd" path="/var/lib/kubelet/pods/f81aa96a-cf8f-43bf-b3cd-709231106cdd/volumes"
May 05 00:43:22 minikube kubelet[2279]: I0505 00:43:22.333632    2279 topology_manager.go:215] "Topology Admit Handler" podUID="589b653d-7565-4528-9e79-c4658702ec83" podNamespace="default" podName="mysql-deployment-77fb8bc69c-wpcx8"
May 05 00:43:22 minikube kubelet[2279]: E0505 00:43:22.333768    2279 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f81aa96a-cf8f-43bf-b3cd-709231106cdd" containerName="wordpress"
May 05 00:43:22 minikube kubelet[2279]: E0505 00:43:22.333786    2279 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="373b80d8-c5cf-48c9-b93b-5a6f07d0b590" containerName="mysql"
May 05 00:43:22 minikube kubelet[2279]: I0505 00:43:22.333817    2279 memory_manager.go:354] "RemoveStaleState removing state" podUID="f81aa96a-cf8f-43bf-b3cd-709231106cdd" containerName="wordpress"
May 05 00:43:22 minikube kubelet[2279]: I0505 00:43:22.333832    2279 memory_manager.go:354] "RemoveStaleState removing state" podUID="373b80d8-c5cf-48c9-b93b-5a6f07d0b590" containerName="mysql"
May 05 00:43:22 minikube kubelet[2279]: I0505 00:43:22.508211    2279 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2\" (UniqueName: \"kubernetes.io/host-path/589b653d-7565-4528-9e79-c4658702ec83-pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2\") pod \"mysql-deployment-77fb8bc69c-wpcx8\" (UID: \"589b653d-7565-4528-9e79-c4658702ec83\") " pod="default/mysql-deployment-77fb8bc69c-wpcx8"
May 05 00:43:22 minikube kubelet[2279]: I0505 00:43:22.508280    2279 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-85r6f\" (UniqueName: \"kubernetes.io/projected/589b653d-7565-4528-9e79-c4658702ec83-kube-api-access-85r6f\") pod \"mysql-deployment-77fb8bc69c-wpcx8\" (UID: \"589b653d-7565-4528-9e79-c4658702ec83\") " pod="default/mysql-deployment-77fb8bc69c-wpcx8"
May 05 00:43:24 minikube kubelet[2279]: I0505 00:43:24.533104    2279 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/mysql-deployment-77fb8bc69c-wpcx8" podStartSLOduration=4.533084978 podStartE2EDuration="4.533084978s" podCreationTimestamp="2024-05-05 00:43:20 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-05 00:43:24.532895668 +0000 UTC m=+231745.381731348" watchObservedRunningTime="2024-05-05 00:43:24.533084978 +0000 UTC m=+231745.381920658"
May 05 00:43:33 minikube kubelet[2279]: I0505 00:43:33.370523    2279 topology_manager.go:215] "Topology Admit Handler" podUID="c5187ae0-6831-435b-bfa5-54db20f6004c" podNamespace="default" podName="wordpress-deployment-5cf59d7c46-ww6kt"
May 05 00:43:33 minikube kubelet[2279]: I0505 00:43:33.490856    2279 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dcskb\" (UniqueName: \"kubernetes.io/projected/c5187ae0-6831-435b-bfa5-54db20f6004c-kube-api-access-dcskb\") pod \"wordpress-deployment-5cf59d7c46-ww6kt\" (UID: \"c5187ae0-6831-435b-bfa5-54db20f6004c\") " pod="default/wordpress-deployment-5cf59d7c46-ww6kt"
May 05 00:43:33 minikube kubelet[2279]: I0505 00:43:33.490934    2279 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4\" (UniqueName: \"kubernetes.io/host-path/c5187ae0-6831-435b-bfa5-54db20f6004c-pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4\") pod \"wordpress-deployment-5cf59d7c46-ww6kt\" (UID: \"c5187ae0-6831-435b-bfa5-54db20f6004c\") " pod="default/wordpress-deployment-5cf59d7c46-ww6kt"
May 05 00:43:34 minikube kubelet[2279]: I0505 00:43:34.722408    2279 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b9b0ba54c372c0c020d7e3312ff12f9f9ad13d9e63fe0c473eb91d5d3c491a25"
May 05 00:43:35 minikube kubelet[2279]: I0505 00:43:35.814686    2279 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/wordpress-deployment-5cf59d7c46-ww6kt" podStartSLOduration=13.814663506 podStartE2EDuration="13.814663506s" podCreationTimestamp="2024-05-05 00:43:22 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-05 00:43:35.813468849 +0000 UTC m=+231756.662304529" watchObservedRunningTime="2024-05-05 00:43:35.814663506 +0000 UTC m=+231756.663499286"
May 05 00:46:25 minikube kubelet[2279]: I0505 00:46:25.119052    2279 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"mysql-persistent-storage\" (UniqueName: \"kubernetes.io/host-path/589b653d-7565-4528-9e79-c4658702ec83-pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2\") pod \"589b653d-7565-4528-9e79-c4658702ec83\" (UID: \"589b653d-7565-4528-9e79-c4658702ec83\") "
May 05 00:46:25 minikube kubelet[2279]: I0505 00:46:25.119142    2279 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-85r6f\" (UniqueName: \"kubernetes.io/projected/589b653d-7565-4528-9e79-c4658702ec83-kube-api-access-85r6f\") pod \"589b653d-7565-4528-9e79-c4658702ec83\" (UID: \"589b653d-7565-4528-9e79-c4658702ec83\") "
May 05 00:46:25 minikube kubelet[2279]: I0505 00:46:25.119237    2279 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/589b653d-7565-4528-9e79-c4658702ec83-pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2" (OuterVolumeSpecName: "mysql-persistent-storage") pod "589b653d-7565-4528-9e79-c4658702ec83" (UID: "589b653d-7565-4528-9e79-c4658702ec83"). InnerVolumeSpecName "pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2". PluginName "kubernetes.io/host-path", VolumeGidValue ""
May 05 00:46:25 minikube kubelet[2279]: I0505 00:46:25.121732    2279 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/589b653d-7565-4528-9e79-c4658702ec83-kube-api-access-85r6f" (OuterVolumeSpecName: "kube-api-access-85r6f") pod "589b653d-7565-4528-9e79-c4658702ec83" (UID: "589b653d-7565-4528-9e79-c4658702ec83"). InnerVolumeSpecName "kube-api-access-85r6f". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 05 00:46:25 minikube kubelet[2279]: I0505 00:46:25.232592    2279 reconciler_common.go:289] "Volume detached for volume \"pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2\" (UniqueName: \"kubernetes.io/host-path/589b653d-7565-4528-9e79-c4658702ec83-pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2\") on node \"minikube\" DevicePath \"\""
May 05 00:46:25 minikube kubelet[2279]: I0505 00:46:25.232632    2279 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-85r6f\" (UniqueName: \"kubernetes.io/projected/589b653d-7565-4528-9e79-c4658702ec83-kube-api-access-85r6f\") on node \"minikube\" DevicePath \"\""
May 05 00:46:25 minikube kubelet[2279]: I0505 00:46:25.535656    2279 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-dcskb\" (UniqueName: \"kubernetes.io/projected/c5187ae0-6831-435b-bfa5-54db20f6004c-kube-api-access-dcskb\") pod \"c5187ae0-6831-435b-bfa5-54db20f6004c\" (UID: \"c5187ae0-6831-435b-bfa5-54db20f6004c\") "
May 05 00:46:25 minikube kubelet[2279]: I0505 00:46:25.535735    2279 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"wordpress-persistent-storage\" (UniqueName: \"kubernetes.io/host-path/c5187ae0-6831-435b-bfa5-54db20f6004c-pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4\") pod \"c5187ae0-6831-435b-bfa5-54db20f6004c\" (UID: \"c5187ae0-6831-435b-bfa5-54db20f6004c\") "
May 05 00:46:25 minikube kubelet[2279]: I0505 00:46:25.535777    2279 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/c5187ae0-6831-435b-bfa5-54db20f6004c-pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4" (OuterVolumeSpecName: "wordpress-persistent-storage") pod "c5187ae0-6831-435b-bfa5-54db20f6004c" (UID: "c5187ae0-6831-435b-bfa5-54db20f6004c"). InnerVolumeSpecName "pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4". PluginName "kubernetes.io/host-path", VolumeGidValue ""
May 05 00:46:25 minikube kubelet[2279]: I0505 00:46:25.538348    2279 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/c5187ae0-6831-435b-bfa5-54db20f6004c-kube-api-access-dcskb" (OuterVolumeSpecName: "kube-api-access-dcskb") pod "c5187ae0-6831-435b-bfa5-54db20f6004c" (UID: "c5187ae0-6831-435b-bfa5-54db20f6004c"). InnerVolumeSpecName "kube-api-access-dcskb". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 05 00:46:25 minikube kubelet[2279]: I0505 00:46:25.636215    2279 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-dcskb\" (UniqueName: \"kubernetes.io/projected/c5187ae0-6831-435b-bfa5-54db20f6004c-kube-api-access-dcskb\") on node \"minikube\" DevicePath \"\""
May 05 00:46:25 minikube kubelet[2279]: I0505 00:46:25.636278    2279 reconciler_common.go:289] "Volume detached for volume \"pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4\" (UniqueName: \"kubernetes.io/host-path/c5187ae0-6831-435b-bfa5-54db20f6004c-pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4\") on node \"minikube\" DevicePath \"\""
May 05 00:46:26 minikube kubelet[2279]: I0505 00:46:26.016261    2279 scope.go:117] "RemoveContainer" containerID="0b81b7525631b4183dcdacb15f8fdeed42b57f4eeb244e70892b7005798f94b5"
May 05 00:46:26 minikube kubelet[2279]: I0505 00:46:26.130520    2279 scope.go:117] "RemoveContainer" containerID="291eade9315345cd07a38c9207fa1794bf6773666db5c9975acde8e2bf2d8c85"
May 05 00:46:26 minikube kubelet[2279]: I0505 00:46:26.190787    2279 scope.go:117] "RemoveContainer" containerID="291eade9315345cd07a38c9207fa1794bf6773666db5c9975acde8e2bf2d8c85"
May 05 00:46:26 minikube kubelet[2279]: E0505 00:46:26.191963    2279 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 291eade9315345cd07a38c9207fa1794bf6773666db5c9975acde8e2bf2d8c85" containerID="291eade9315345cd07a38c9207fa1794bf6773666db5c9975acde8e2bf2d8c85"
May 05 00:46:26 minikube kubelet[2279]: I0505 00:46:26.192003    2279 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"291eade9315345cd07a38c9207fa1794bf6773666db5c9975acde8e2bf2d8c85"} err="failed to get container status \"291eade9315345cd07a38c9207fa1794bf6773666db5c9975acde8e2bf2d8c85\": rpc error: code = Unknown desc = Error response from daemon: No such container: 291eade9315345cd07a38c9207fa1794bf6773666db5c9975acde8e2bf2d8c85"
May 05 00:46:27 minikube kubelet[2279]: I0505 00:46:27.447901    2279 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="589b653d-7565-4528-9e79-c4658702ec83" path="/var/lib/kubelet/pods/589b653d-7565-4528-9e79-c4658702ec83/volumes"
May 05 00:46:27 minikube kubelet[2279]: I0505 00:46:27.448744    2279 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="c5187ae0-6831-435b-bfa5-54db20f6004c" path="/var/lib/kubelet/pods/c5187ae0-6831-435b-bfa5-54db20f6004c/volumes"
May 05 00:46:32 minikube kubelet[2279]: I0505 00:46:32.449986    2279 topology_manager.go:215] "Topology Admit Handler" podUID="30761d75-fd58-40ef-ad91-0ca921864aa8" podNamespace="default" podName="mysql-deployment-77fb8bc69c-w4mqc"
May 05 00:46:32 minikube kubelet[2279]: E0505 00:46:32.450085    2279 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c5187ae0-6831-435b-bfa5-54db20f6004c" containerName="wordpress"
May 05 00:46:32 minikube kubelet[2279]: E0505 00:46:32.450108    2279 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="589b653d-7565-4528-9e79-c4658702ec83" containerName="mysql"
May 05 00:46:32 minikube kubelet[2279]: I0505 00:46:32.450145    2279 memory_manager.go:354] "RemoveStaleState removing state" podUID="c5187ae0-6831-435b-bfa5-54db20f6004c" containerName="wordpress"
May 05 00:46:32 minikube kubelet[2279]: I0505 00:46:32.450160    2279 memory_manager.go:354] "RemoveStaleState removing state" podUID="589b653d-7565-4528-9e79-c4658702ec83" containerName="mysql"
May 05 00:46:32 minikube kubelet[2279]: I0505 00:46:32.592146    2279 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bx2r4\" (UniqueName: \"kubernetes.io/projected/30761d75-fd58-40ef-ad91-0ca921864aa8-kube-api-access-bx2r4\") pod \"mysql-deployment-77fb8bc69c-w4mqc\" (UID: \"30761d75-fd58-40ef-ad91-0ca921864aa8\") " pod="default/mysql-deployment-77fb8bc69c-w4mqc"
May 05 00:46:32 minikube kubelet[2279]: I0505 00:46:32.592275    2279 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-31177d15-5585-49e9-8868-06df8b432d36\" (UniqueName: \"kubernetes.io/host-path/30761d75-fd58-40ef-ad91-0ca921864aa8-pvc-31177d15-5585-49e9-8868-06df8b432d36\") pod \"mysql-deployment-77fb8bc69c-w4mqc\" (UID: \"30761d75-fd58-40ef-ad91-0ca921864aa8\") " pod="default/mysql-deployment-77fb8bc69c-w4mqc"
May 05 00:47:48 minikube kubelet[2279]: I0505 00:47:48.520140    2279 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/mysql-deployment-77fb8bc69c-w4mqc" podStartSLOduration=83.520118549 podStartE2EDuration="1m23.520118549s" podCreationTimestamp="2024-05-05 00:46:25 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-05 00:46:34.196068401 +0000 UTC m=+231935.052402600" watchObservedRunningTime="2024-05-05 00:47:48.520118549 +0000 UTC m=+232009.380279735"
May 05 00:47:48 minikube kubelet[2279]: I0505 00:47:48.520660    2279 topology_manager.go:215] "Topology Admit Handler" podUID="75844fd1-b7d3-4329-8b00-cdbbe102bcb6" podNamespace="default" podName="wordpress-deployment-6777f48f9b-fz8p9"
May 05 00:47:48 minikube kubelet[2279]: I0505 00:47:48.631141    2279 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bfhf2\" (UniqueName: \"kubernetes.io/projected/75844fd1-b7d3-4329-8b00-cdbbe102bcb6-kube-api-access-bfhf2\") pod \"wordpress-deployment-6777f48f9b-fz8p9\" (UID: \"75844fd1-b7d3-4329-8b00-cdbbe102bcb6\") " pod="default/wordpress-deployment-6777f48f9b-fz8p9"
May 05 00:47:48 minikube kubelet[2279]: I0505 00:47:48.631240    2279 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-c475f63f-0e5e-4f60-b923-2e34e1518c08\" (UniqueName: \"kubernetes.io/host-path/75844fd1-b7d3-4329-8b00-cdbbe102bcb6-pvc-c475f63f-0e5e-4f60-b923-2e34e1518c08\") pod \"wordpress-deployment-6777f48f9b-fz8p9\" (UID: \"75844fd1-b7d3-4329-8b00-cdbbe102bcb6\") " pod="default/wordpress-deployment-6777f48f9b-fz8p9"
May 05 00:47:50 minikube kubelet[2279]: I0505 00:47:50.264856    2279 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/wordpress-deployment-6777f48f9b-fz8p9" podStartSLOduration=6.264823608 podStartE2EDuration="6.264823608s" podCreationTimestamp="2024-05-05 00:47:44 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-05 00:47:50.263845758 +0000 UTC m=+232011.124006944" watchObservedRunningTime="2024-05-05 00:47:50.264823608 +0000 UTC m=+232011.124984794"


==> storage-provisioner [12df540ec5f6] <==
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0005200e0, 0x18b3d60, 0xc000313e90, 0x1, 0xc0000c2ba0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0005200e0, 0x3b9aca00, 0x0, 0x17a0501, 0xc0000c2ba0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0005200e0, 0x3b9aca00, 0xc0000c2ba0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 130957 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0xc0004c1220, 0x2)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0004c1210)
	/usr/local/go/src/sync/cond.go:56 +0x99
golang.org/x/net/http2.(*pipe).Read(0xc0004c1208, 0xc000496001, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x97
golang.org/x/net/http2.transportResponseBody.Read(0xc0004c11e0, 0xc000496001, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0xaf
encoding/json.(*Decoder).refill(0xc0004c14a0, 0xa, 0x9)
	/usr/local/go/src/encoding/json/stream.go:165 +0xeb
encoding/json.(*Decoder).readValue(0xc0004c14a0, 0x0, 0x0, 0x152aee0)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1ff
encoding/json.(*Decoder).Decode(0xc0004c14a0, 0x154a160, 0xc0003f8cc0, 0x203000, 0x203000)
	/usr/local/go/src/encoding/json/stream.go:63 +0x7c
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0xc000281260, 0xc000220000, 0x400, 0x400, 0x40, 0x38, 0x15b0440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x1a8
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc0002840f0, 0x0, 0x18bc168, 0xc0006f3700, 0x0, 0x0, 0x461dc0, 0xc000510540, 0xc00008fe50)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc000520f80, 0xc00008fef0, 0x8, 0x18baa20, 0xc0003e2f00, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc00007f640)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

goroutine 131101 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0xc00025c5c0, 0x1)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc00025c5b0)
	/usr/local/go/src/sync/cond.go:56 +0x99
golang.org/x/net/http2.(*pipe).Read(0xc00025c5a8, 0xc00047e001, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x97
golang.org/x/net/http2.transportResponseBody.Read(0xc00025c580, 0xc00047e001, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0xaf
encoding/json.(*Decoder).refill(0xc00025c840, 0xa, 0x9)
	/usr/local/go/src/encoding/json/stream.go:165 +0xeb
encoding/json.(*Decoder).readValue(0xc00025c840, 0x0, 0x0, 0x152aee0)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1ff
encoding/json.(*Decoder).Decode(0xc00025c840, 0x154a160, 0xc00000c8d0, 0x203000, 0x203000)
	/usr/local/go/src/encoding/json/stream.go:63 +0x7c
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0xc0001ebb90, 0xc0006c5c00, 0x400, 0x400, 0x40, 0x38, 0x15b0440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x1a8
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc000674fa0, 0x0, 0x18bc168, 0xc000131540, 0x0, 0x0, 0x461dc0, 0xc0007518c0, 0xc0004d1e50)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc0000970e0, 0xc0004d1ef0, 0x8, 0x18bbba0, 0xc00016f980, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc000224d80)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe


==> storage-provisioner [69226188fd1c] <==
I0505 00:33:55.440262       1 controller.go:1542] delete "pvc-216083ba-3722-4fce-aaf0-ff24d11ca486": succeeded
I0505 00:39:46.582342       1 controller.go:1472] delete "pvc-06689616-9984-48cb-8682-352cb0daec1b": started
I0505 00:39:46.582442       1 controller.go:1472] delete "pvc-e6191fdd-7ab7-47b8-a192-4142cf37c720": started
I0505 00:39:46.582543       1 controller.go:1472] delete "pvc-fb435956-acaf-4cc3-8272-28f2292dbb3d": started
I0505 00:39:46.583152       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-fb435956-acaf-4cc3-8272-28f2292dbb3d    30362c88-c6cf-4af9-ace6-421a7da19257 96437 0 2024-05-02 17:42:56 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:05c2f7c8-8399-4e61-9d1e-16a2900a41d5 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-05-02 17:42:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-05-03 13:06:47 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{21474836480 0} {<nil>} 20Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/mysql-pv-claim,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:mysql-pv-claim,UID:fb435956-acaf-4cc3-8272-28f2292dbb3d,APIVersion:v1,ResourceVersion:30551,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0505 00:39:46.583297       1 controller.go:1478] delete "pvc-fb435956-acaf-4cc3-8272-28f2292dbb3d": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0505 00:39:46.582393       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-06689616-9984-48cb-8682-352cb0daec1b    43815e09-db49-4abb-b8da-f5ded197c6dd 96439 0 2024-05-02 10:42:38 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:05c2f7c8-8399-4e61-9d1e-16a2900a41d5 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-05-02 10:42:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-05-03 13:06:47 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{1073741824 0} {<nil>} 1Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/mysql-pvc,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:mysql-pvc,UID:06689616-9984-48cb-8682-352cb0daec1b,APIVersion:v1,ResourceVersion:7733,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0505 00:39:46.583362       1 controller.go:1478] delete "pvc-06689616-9984-48cb-8682-352cb0daec1b": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0505 00:39:46.582968       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-e6191fdd-7ab7-47b8-a192-4142cf37c720    eda18627-cfac-4975-a978-af52405e72ef 96443 0 2024-05-02 17:43:21 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:05c2f7c8-8399-4e61-9d1e-16a2900a41d5 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-05-02 17:43:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-05-03 13:06:48 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{21474836480 0} {<nil>} 20Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/wp-pv-claim,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:wp-pv-claim,UID:e6191fdd-7ab7-47b8-a192-4142cf37c720,APIVersion:v1,ResourceVersion:30590,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0505 00:39:46.583691       1 controller.go:1478] delete "pvc-e6191fdd-7ab7-47b8-a192-4142cf37c720": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0505 00:43:21.723074       1 controller.go:1332] provision "default/mysql-pvc" class "standard": started
I0505 00:43:21.723147       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    bc26d65f-af75-4b88-9ebd-a08c9ba0720b 311 0 2024-05-02 08:18:09 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2024-05-02 08:18:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2 &PersistentVolumeClaim{ObjectMeta:{mysql-pvc  default  e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2 223048 0 2024-05-05 00:43:21 +0000 UTC <nil> <nil> map[] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"mysql-pvc","namespace":"default"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"cpu":"100m","storage":"2Gi"}}}}
 volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2024-05-05 00:43:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}}}}} {kubectl-client-side-apply Update v1 2024-05-05 00:43:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:cpu":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/mysql-pvc
I0505 00:43:21.724087       1 controller.go:1439] provision "default/mysql-pvc" class "standard": volume "pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2" provisioned
I0505 00:43:21.724112       1 controller.go:1456] provision "default/mysql-pvc" class "standard": succeeded
I0505 00:43:21.724119       1 volume_store.go:212] Trying to save persistentvolume "pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2"
I0505 00:43:21.724891       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mysql-pvc", UID:"e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2", APIVersion:"v1", ResourceVersion:"223048", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/mysql-pvc"
I0505 00:43:21.744560       1 volume_store.go:219] persistentvolume "pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2" saved
I0505 00:43:21.744908       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mysql-pvc", UID:"e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2", APIVersion:"v1", ResourceVersion:"223048", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2
I0505 00:43:31.154016       1 controller.go:1332] provision "default/wordpress-pvc" class "standard": started
I0505 00:43:31.154611       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    bc26d65f-af75-4b88-9ebd-a08c9ba0720b 311 0 2024-05-02 08:18:09 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2024-05-02 08:18:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4 &PersistentVolumeClaim{ObjectMeta:{wordpress-pvc  default  21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4 223093 0 2024-05-05 00:43:31 +0000 UTC <nil> <nil> map[] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"wordpress-pvc","namespace":"default"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"2Gi"}}}}
 volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2024-05-05 00:43:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}}}}} {kubectl-client-side-apply Update v1 2024-05-05 00:43:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/wordpress-pvc
I0505 00:43:31.155476       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"wordpress-pvc", UID:"21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4", APIVersion:"v1", ResourceVersion:"223093", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/wordpress-pvc"
I0505 00:43:31.157838       1 controller.go:1439] provision "default/wordpress-pvc" class "standard": volume "pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4" provisioned
I0505 00:43:31.159017       1 controller.go:1456] provision "default/wordpress-pvc" class "standard": succeeded
I0505 00:43:31.164411       1 volume_store.go:212] Trying to save persistentvolume "pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4"
I0505 00:43:31.250282       1 volume_store.go:219] persistentvolume "pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4" saved
I0505 00:43:31.250936       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"wordpress-pvc", UID:"21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4", APIVersion:"v1", ResourceVersion:"223093", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4
I0505 00:46:26.225143       1 controller.go:1472] delete "pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4": started
I0505 00:46:26.225167       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4    09a317c0-54f9-41bf-b98d-c1ed7de05f52 223320 0 2024-05-05 00:43:31 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:7cabd5c9-699e-4c72-bdd3-6536328b8365 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-05-05 00:43:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-05-05 00:46:26 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/wordpress-pvc,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:wordpress-pvc,UID:21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4,APIVersion:v1,ResourceVersion:223093,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0505 00:46:26.260019       1 controller.go:1472] delete "pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2": started
I0505 00:46:26.260046       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2    34cc3d34-310d-4d6b-8751-7f887106d297 223323 0 2024-05-05 00:43:21 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:7cabd5c9-699e-4c72-bdd3-6536328b8365 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-05-05 00:43:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-05-05 00:46:26 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/mysql-pvc,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:mysql-pvc,UID:e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2,APIVersion:v1,ResourceVersion:223048,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0505 00:46:26.284980       1 controller.go:1487] delete "pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2": volume deleted
I0505 00:46:26.302405       1 controller.go:1537] delete "pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2": persistentvolume deleted
I0505 00:46:26.302594       1 controller.go:1542] delete "pvc-e5a1dbc9-c75e-4681-9ee4-24b8ef2daaf2": succeeded
I0505 00:46:26.376559       1 controller.go:1487] delete "pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4": volume deleted
I0505 00:46:26.385394       1 controller.go:1537] delete "pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4": persistentvolume deleted
I0505 00:46:26.385452       1 controller.go:1542] delete "pvc-21b0f3f2-e830-4cd4-b8ba-d2d735f0e7c4": succeeded
I0505 00:46:28.992202       1 controller.go:1332] provision "default/mysql-pvc" class "standard": started
I0505 00:46:28.992415       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    bc26d65f-af75-4b88-9ebd-a08c9ba0720b 311 0 2024-05-02 08:18:09 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2024-05-02 08:18:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-31177d15-5585-49e9-8868-06df8b432d36 &PersistentVolumeClaim{ObjectMeta:{mysql-pvc  default  31177d15-5585-49e9-8868-06df8b432d36 223333 0 2024-05-05 00:46:28 +0000 UTC <nil> <nil> map[] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"mysql-pvc","namespace":"default"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"cpu":"100m","storage":"2Gi"}}}}
 volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2024-05-05 00:46:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}}}}} {kubectl-client-side-apply Update v1 2024-05-05 00:46:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:cpu":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/mysql-pvc
I0505 00:46:28.992998       1 controller.go:1439] provision "default/mysql-pvc" class "standard": volume "pvc-31177d15-5585-49e9-8868-06df8b432d36" provisioned
I0505 00:46:28.993026       1 controller.go:1456] provision "default/mysql-pvc" class "standard": succeeded
I0505 00:46:28.993034       1 volume_store.go:212] Trying to save persistentvolume "pvc-31177d15-5585-49e9-8868-06df8b432d36"
I0505 00:46:28.993481       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mysql-pvc", UID:"31177d15-5585-49e9-8868-06df8b432d36", APIVersion:"v1", ResourceVersion:"223333", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/mysql-pvc"
I0505 00:46:29.013957       1 volume_store.go:219] persistentvolume "pvc-31177d15-5585-49e9-8868-06df8b432d36" saved
I0505 00:46:29.014105       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mysql-pvc", UID:"31177d15-5585-49e9-8868-06df8b432d36", APIVersion:"v1", ResourceVersion:"223333", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-31177d15-5585-49e9-8868-06df8b432d36
I0505 00:47:46.006639       1 controller.go:1332] provision "default/wordpress-pvc" class "standard": started
I0505 00:47:46.008596       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    bc26d65f-af75-4b88-9ebd-a08c9ba0720b 311 0 2024-05-02 08:18:09 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2024-05-02 08:18:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-c475f63f-0e5e-4f60-b923-2e34e1518c08 &PersistentVolumeClaim{ObjectMeta:{wordpress-pvc  default  c475f63f-0e5e-4f60-b923-2e34e1518c08 223445 0 2024-05-05 00:47:45 +0000 UTC <nil> <nil> map[] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"wordpress-pvc","namespace":"default"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"2Gi"}}}}
 volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2024-05-05 00:47:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}}}}} {kubectl-client-side-apply Update v1 2024-05-05 00:47:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/wordpress-pvc
I0505 00:47:46.014387       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"wordpress-pvc", UID:"c475f63f-0e5e-4f60-b923-2e34e1518c08", APIVersion:"v1", ResourceVersion:"223445", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/wordpress-pvc"
I0505 00:47:46.027085       1 controller.go:1439] provision "default/wordpress-pvc" class "standard": volume "pvc-c475f63f-0e5e-4f60-b923-2e34e1518c08" provisioned
I0505 00:47:46.027143       1 controller.go:1456] provision "default/wordpress-pvc" class "standard": succeeded
I0505 00:47:46.027153       1 volume_store.go:212] Trying to save persistentvolume "pvc-c475f63f-0e5e-4f60-b923-2e34e1518c08"
I0505 00:47:46.140866       1 volume_store.go:219] persistentvolume "pvc-c475f63f-0e5e-4f60-b923-2e34e1518c08" saved
I0505 00:47:46.153883       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"wordpress-pvc", UID:"c475f63f-0e5e-4f60-b923-2e34e1518c08", APIVersion:"v1", ResourceVersion:"223445", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-c475f63f-0e5e-4f60-b923-2e34e1518c08

